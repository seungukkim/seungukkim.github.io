{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/06/17/hello-world/"},{"title":"Day-1 Visualization","text":"R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. ggplot2 시각화 다음과 같이 시각화를 작성한다. 1234library(ggplot2)ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point()","link":"/2022/06/17/day0617/"},{"title":"test","text":"##csv 파일 불러오기-csv 파일을 불러옵니다. 12mpg1&lt;-read.csv(&quot;mpg1.csv&quot;)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... ##데이터 시각화 하기-cty,hwy 산점도를 그려본다. 1234library(ggplot2)ggplot(mpg1,aes(x=cty, y=hwy))+ geom_point()","link":"/2022/06/20/test/"},{"title":"test2","text":"이 상황에서 knit를 누른다(그림은 눌린 이후의 상태이다.) 가장 중요한 것은 mpg1.csv와 testRMd 가 같이 있어야한다. rmd_0620에 있는 test_files 안에 있는 이미지 파일을 복사한다. 그리고 blog ⇒ source⇒ images ⇒rmd_0620에 붙여 넣는다. 그리고 test(Markdown 원본 파일)을 blog ⇒ source⇒_posts 안에 넣는다. 원래 test md에 있는 이미지의 경로를 지우고, images 안에 있는 unnamed-chunk의 경로를 넣는다.","link":"/2022/06/20/test2/"},{"title":"day0621","text":"##빈도의 12mpg1&lt;-read.csv(&quot;mpg1.csv&quot;, stringsAsFactors = F)str(mpg1) 123456## 'data.frame': 234 obs. of 5 variables:## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ...## $ trans : chr &quot;auto&quot; &quot;manual&quot; &quot;manual&quot; &quot;auto&quot; ...## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ...## $ cty : int 18 21 20 21 16 18 18 18 16 20 ...## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... #1. iris 데이터세트에 있는 변수 Petal.Width의 평균, 최댓값, 최솟값을 구해보세요. 1mean(iris$Petal.Width) 1## [1] 1.199333 1max(iris$Petal.Width) 1## [1] 2.5 1min(iris$Petal.Width) 1## [1] 0.1 #2. ggplot2 패키지에 있는 mpg 데이터세트에서 자동차 class의 자동차 빈도수와 비율을 구하세요. 비율은 백분율이며, 소수점 한자리까지 구합니다. 123library(ggplot2)c&lt;-table(mpg$class)round(prop.table(c)*100,1) 123## ## 2seater compact midsize minivan pickup subcompact suv ## 2.1 20.1 17.5 4.7 14.1 15.0 26.5 #3. mpg에서 자동차 class에 따른 drv의 빈도와 백분율을 구합니다. 조건은 class 별로 drv의 백분율을 계산합니다. 소수점 한자리까지 구합니다. 123library(ggplot2)d&lt;-table(mpg$class,mpg$drv)round(prop.table(d)*100,1) 123456789## ## 4 f r## 2seater 0.0 0.0 2.1## compact 5.1 15.0 0.0## midsize 1.3 16.2 0.0## minivan 0.0 4.7 0.0## pickup 14.1 0.0 0.0## subcompact 1.7 9.4 3.8## suv 21.8 0.0 4.7 #summary 1summary(iris) 1234567891011121314## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 1summary(iris$Petal.Width) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.100 0.300 1.300 1.199 1.800 2.500 #quantile(), IQR 1quantile(mtcars$mpg) 12## 0% 25% 50% 75% 100% ## 10.400 15.425 19.200 22.800 33.900 1IQR(mtcars$mpg) 1## [1] 7.375","link":"/2022/06/21/day0621/"},{"title":"day0622","text":"##라이브러리 불러오기 1library(dplyr) 12## ## 다음의 패키지를 부착합니다: 'dplyr' 123## The following objects are masked from 'package:stats':## ## filter, lag 123## The following objects are masked from 'package:base':## ## intersect, setdiff, setequal, union 1library(ggplot2) ##데이터를 불러오세요 1getwd() 1## [1] &quot;C:/Users/human/Desktop/R_lecture/day0622&quot; 12exam_na&lt;-read.csv(&quot;../data/exam_na.csv&quot;) is.na(exam_na) 123456## id sex korean english math## [1,] FALSE FALSE FALSE TRUE FALSE## [2,] FALSE FALSE FALSE FALSE FALSE## [3,] FALSE FALSE FALSE FALSE FALSE## [4,] FALSE FALSE TRUE FALSE FALSE## [5,] FALSE FALSE FALSE TRUE FALSE ##결측치의 빈도를 구하기#데이터세트 전체의 결측치 빈도를 확인#korean 변수의 결측치 빈도를 확인 1table(is.na(exam_na)) 123## ## FALSE TRUE ## 22 3 1table(is.na(exam_na$korean)) 123## ## FALSE TRUE ## 4 1 1summary(is.na(exam_na)) 12345678## id sex korean english ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:5 FALSE:5 FALSE:4 FALSE:3 ## TRUE :1 TRUE :2 ## math ## Mode :logical ## FALSE:5 ## 1summary(exam_na) 12345678## id sex korean english math ## Min. :1 Length:5 Min. :87.00 Min. :84.00 Min. :80.0 ## 1st Qu.:2 Class :character 1st Qu.:87.00 1st Qu.:88.00 1st Qu.:82.0 ## Median :3 Mode :character Median :89.50 Median :92.00 Median :88.0 ## Mean :3 Mean :90.25 Mean :90.33 Mean :86.6 ## 3rd Qu.:4 3rd Qu.:92.75 3rd Qu.:93.50 3rd Qu.:90.0 ## Max. :5 Max. :95.00 Max. :95.00 Max. :93.0 ## NA's :1 NA's :2 ##결측치 처리 방법-제거하고 처리하기 -다른 값으로 대체하기 +평균 입력 (1) 결측치를 제외하고 분석하기na.rm = T#제거해라na.rm = F #제거하지마라 1mean(exam_na$korean,na.rm = T) #결측치를 제거하고 평균을 구해라 1## [1] 90.25 -na.omit()-결측치가 있는 행을 모두 제거.+가급적 쓰지 말것 -filter() 활용 1exam_na %&gt;% filter(is.na(korean)) 12## id sex korean english math## 1 4 M NA 84 80 1exam_na %&gt;% filter(!is.na(korean)) 12345## id sex korean english math## 1 1 M 87 NA 82## 2 2 F 92 95 93## 3 3 F 95 92 90## 4 5 F 87 NA 88 ###결측치를 다른 값으로 대체하기-imputation 결측치 제거, 대체 링크 ##이상치-데이터의 특정 값이 뭔가 ‘이상’이 있다.-Case 1: 정해진 범주에서 벗어난 데이터 +2000년 4월 30일 / 2000년 40월 30일 —-&gt;9999로 처리하자-Case 2: 숫자 / 아웃라이어(Outlier) / 극단값 12mpg1_out &lt;-read.csv(&quot;../data/mpg1_out.csv&quot;)glimpse(mpg1_out) 12345## Rows: 234## Columns: 3## $ trans &lt;int&gt; 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1…## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;,…## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 15, 15, … -trans의 갯수를 구하자 1table(mpg1_out$trans) 123## ## 1 2 3 ## 154 76 4 -ifelse-만약 ~라면, 내가 무엇을 할 것이다.-만약 trans의 값이 3이라면, 결측치로 바꿔주세요. 나머지는 그대로 유지하세요 12mpg1_out$trans&lt;-ifelse(mpg1_out$trans==3,NA,mpg1_out$trans)table(is.na(mpg1_out$trans)) 123## ## FALSE TRUE ## 230 4 -결측치 제거 12result&lt;-mpg1_out %&gt;% filter(!is.na(trans))table(is.na(result$trans)) 123## ## FALSE ## 230 1mpg1_out %&gt;% filter(trans!=3) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231## trans drv cty## 1 1 f 18## 2 2 f 21## 3 2 f 20## 4 1 f 21## 5 1 f 16## 6 2 f 18## 7 1 f 18## 8 2 4 18## 9 1 4 16## 10 2 4 20## 11 1 4 19## 12 1 4 15## 13 2 5 17## 14 1 4 17## 15 2 4 15## 16 1 4 15## 17 1 4 17## 18 1 4 16## 19 1 r 14## 20 1 r 14## 21 1 r 13## 22 1 r 12## 23 2 r 16## 24 1 r 15## 25 2 r 16## 26 1 r 15## 27 1 4 14## 28 1 4 11## 29 1 4 11## 30 1 4 14## 31 1 f 19## 32 1 f 22## 33 1 f 18## 34 1 f 18## 35 1 f 17## 36 1 f 18## 37 1 f 17## 38 1 f 16## 39 1 f 16## 40 1 f 17## 41 1 f 17## 42 1 f 11## 43 1 f 15## 44 1 f 16## 45 1 f 16## 46 2 4 15## 47 1 4 14## 48 1 4 13## 49 2 4 14## 50 1 4 14## 51 1 4 14## 52 1 5 9## 53 2 4 11## 54 1 4 11## 55 1 4 13## 56 1 4 13## 57 1 4 9## 58 1 4 11## 59 1 4 13## 60 1 4 11## 61 2 4 12## 62 1 4 9## 63 1 4 13## 64 1 4 13## 65 2 4 12## 66 2 4 9## 67 1 4 11## 68 2 4 11## 69 1 4 13## 70 1 4 11## 71 1 r 11## 72 1 r 11## 73 1 r 12## 74 1 4 14## 75 2 4 15## 76 1 4 14## 77 1 4 13## 78 1 4 13## 79 1 5 13## 80 1 4 14## 81 2 4 14## 82 2 4 13## 83 1 4 13## 84 1 4 13## 85 1 4 11## 86 1 4 13## 87 2 r 18## 88 1 r 18## 89 2 r 17## 90 1 r 16## 91 1 r 15## 92 2 r 15## 93 2 r 15## 94 1 r 15## 95 2 r 14## 96 2 f 28## 97 1 f 24## 98 2 f 25## 99 2 f 23## 100 1 f 24## 101 2 f 26## 102 1 f 25## 103 1 f 24## 104 2 f 21## 105 1 f 18## 106 2 f 18## 107 1 f 21## 108 2 f 21## 109 1 f 18## 110 2 f 18## 111 1 f 19## 112 1 f 19## 113 2 f 19## 114 2 f 20## 115 1 f 20## 116 1 f 17## 117 2 f 16## 118 2 f 17## 119 1 4 17## 120 1 4 15## 121 1 4 15## 122 1 4 14## 123 1 4 9## 124 1 4 14## 125 1 4 13## 126 1 4 11## 127 1 4 11## 128 1 4 12## 129 1 4 12## 130 1 4 11## 131 1 r 11## 132 1 r 11## 133 1 r 12## 134 1 4 14## 135 1 4 13## 136 1 4 13## 137 1 4 13## 138 2 f 21## 139 1 f 19## 140 1 f 23## 141 2 f 23## 142 2 f 19## 143 1 f 19## 144 1 f 18## 145 2 f 19## 146 1 f 19## 147 1 4 14## 148 2 4 15## 149 1 4 14## 150 1 4 12## 151 1 f 18## 152 1 f 16## 153 1 f 17## 154 1 f 18## 155 1 f 16## 156 2 4 18## 157 1 4 18## 158 2 4 20## 159 2 4 19## 160 1 4 20## 161 1 4 18## 162 1 4 21## 163 2 4 19## 164 2 4 19## 165 1 4 19## 166 1 4 20## 167 1 4 20## 168 2 4 19## 169 2 4 20## 170 2 4 15## 171 1 4 16## 172 1 4 15## 173 2 4 15## 174 1 4 16## 175 1 4 14## 176 2 f 21## 177 1 f 21## 178 2 f 21## 179 1 f 21## 180 1 f 18## 181 2 f 18## 182 1 f 19## 183 1 f 21## 184 2 f 21## 185 2 f 21## 186 1 f 22## 187 1 f 18## 188 2 f 18## 189 1 f 18## 190 1 f 24## 191 1 f 24## 192 2 f 26## 193 2 f 28## 194 1 f 26## 195 1 4 11## 196 1 4 13## 197 2 4 15## 198 1 4 16## 199 2 4 17## 200 2 4 15## 201 1 4 15## 202 2 4 15## 203 1 4 16## 204 2 f 21## 205 1 f 19## 206 2 f 21## 207 1 f 22## 208 2 f 17## 209 2 f 33## 210 2 f 21## 211 1 f 19## 212 1 f 22## 213 2 f 21## 214 1 f 21## 215 2 f 21## 216 1 f 16## 217 2 f 17## 218 2 f 35## 219 1 f 29## 220 2 f 21## 221 1 f 19## 222 2 f 20## 223 1 f 20## 224 2 f 21## 225 1 f 18## 226 1 f 19## 227 2 f 21## 228 1 f 16## 229 2 f 18## 230 1 f 17 ###극단치 처피-숫자 데이터 boxplot()-boxplot() 함수를 통해서 극단치가 있는지 없는지 확인 가능-IQR:3사분위 - 1사부누이-경계값: IQR+ IQR * 1.5 상한 / IQR-IQR * 1.5 하한 123mpg1&lt;-read.csv(&quot;../data/mpg1.csv&quot;)boxplot(mpg1$cty)boxplot(mpg1$cty)$stats 123456## [,1]## [1,] 9## [2,] 14## [3,] 17## [4,] 19## [5,] 26 1boxplot(mpg1$cty, mpg1$hwy)$stats 123456## [,1] [,2]## [1,] 9 12## [2,] 14 18## [3,] 17 24## [4,] 19 27## [5,] 26 37 1boxplot(mpg1$cty~mpg1$drv)$stats 123456## [,1] [,2] [,3]## [1,] 9 15 11## [2,] 13 18 12## [3,] 14 19 15## [4,] 16 21 15## [5,] 20 25 18 12mpg1$cty&lt;-ifelse(mpg1$cty&gt;26|mpg1$cty&lt;9,NA,mpg1$cty)table(is.na(mpg1$cty)) 123## ## FALSE TRUE ## 229 5 1mean(mpg1$cty,na.rm=T) 1## [1] 16.55895 ##ggplot2 강의-데이터 불러오기 12345678library(readxl)library(ggplot2)who_disease&lt;-read_xlsx(&quot;../data/who_disease.xlsx&quot;)#기본 시각화ggplot(who_disease, aes(x=year,y=cases))+ #그래프의 종류 geom_point(alpha=0.1) 1234#옵션ggplot(who_disease, aes(x=year,y=cases))+ #그래프의 종류 geom_point(alpha=0.1) 1234#옵션 2 색상ggplot(who_disease, aes(x=year,y=cases))+ #그래프의 종류 geom_point(alpha=0.1,colour=&quot;red&quot;) 1234#옵션 2 색상ggplot(who_disease, aes(x=year,y=cases))+ #그래프의 종류 geom_point(alpha=0.1,colour=&quot;12E6c2&quot;,) =colour 입력 위치=geom_point(colour) +aes(x,y,colour =컬럼) 1str(iris) 123456## 'data.frame': 150 obs. of 5 variables:## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 1234ggplot(iris,aes(x=Sepal.Length, y=Sepal.Width, size=Petal.Length))+ geom_point() #산점도: x축 수치형 연속형 데이터, y축 수치형 연속형 데이터# 12ggplot(who_disease,aes(x=year,y=cases))+ geom_point(alpha=0.1) #히스토그램+질병데이터 region=AMR, year=1980,disease, 백일해 12library(dplyr)str(who_disease) 1234567## tibble [43,262 × 6] (S3: tbl_df/tbl/data.frame)## $ region : chr [1:43262] &quot;EMR&quot; &quot;EUR&quot; &quot;AFR&quot; &quot;EUR&quot; ...## $ countryCode: chr [1:43262] &quot;AFG&quot; &quot;ALB&quot; &quot;DZA&quot; &quot;AND&quot; ...## $ country : chr [1:43262] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Andorra&quot; ...## $ disease : chr [1:43262] &quot;measles&quot; &quot;measles&quot; &quot;measles&quot; &quot;measles&quot; ...## $ year : num [1:43262] 2016 2016 2016 2016 2016 ...## $ cases : num [1:43262] 638 17 41 0 53 0 0 2 99 27 ... 123456who_disease %&gt;% filter(region=='AMR', year==1980, disease=='pertussis', cases&gt;0)-&gt;data2ggplot(data2,aes(x=cases))+ geom_histogram(fill=&quot;red&quot;) 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 123ggplot(data2,aes(x=country, y=cases))+ geom_col(fill=&quot;blue&quot;)+ coord_flip() 12ggplot(who_disease,aes(region))+ geom_bar() 12ggplot(who_disease,aes(disease))+ geom_bar() 123library(ggplot2)diamonds&lt;-ggplot2::diamondsggplot(diamonds,aes(x=carat,y=price)) 1ggplot(diamonds,aes(x=carat,y=price))+geom_point() 12ggplot(diamonds, aes(x=cut))+ geom_bar() #table()함수로 cut별 빈도수를 확인 1table(diamonds$cut) 123## ## Fair Good Very Good Premium Ideal ## 1610 4906 12082 13791 21551 12ggplot(diamonds,aes(x=cut,y=price))+ geom_bar(stat=&quot;identity&quot;) #cut 범주별 평균 가격 구하기 1234cut_price&lt;-diamonds %&gt;% group_by(cut) %&gt;% summarise(mean_price=mean(price))cut_price 12345678## # A tibble: 5 × 2## cut mean_price## &lt;ord&gt; &lt;dbl&gt;## 1 Fair 4359.## 2 Good 3929.## 3 Very Good 3982.## 4 Premium 4584.## 5 Ideal 3458. 12ggplot(data=cut_price,aes(x=cut,y=mean_price))+ geom_col() 12ggplot(data=diamonds,aes(x=carat))+ geom_histogram() 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 1range(diamonds$carat) 1## [1] 0.20 5.01 12ggplot(diamonds,aes(x=carat))+ geom_histogram(binwidth = 1) 12ggplot(diamonds,aes(x=carat))+ geom_histogram(binwidth = 0.01) 12ggplot(diamonds,aes(x=carat))+ geom_density() 12ggplot(diamonds,aes(x=1,y=price))+ geom_boxplot() 12ggplot(cars,aes(x=speed,y=dist))+ geom_line() #실업자수 변화 12ggplot(economics,aes(x=date,y=unemploy))+ geom_line() ##ggplot()정교하게 그리기 -산점도 그리기 12ggplot(diamonds, aes(x=carat, y=price, col=cut))+ geom_point() #막대그래프에 2개 범주내용 반영하기 12ggplot(diamonds, aes(x=color,fill=cut))+ geom_bar(position=&quot;fill&quot;) #p.219선 그래프에 2개 범주 내용 반영 12leisure&lt;-read.csv(&quot;../data/leisure.csv&quot;)str(leisure) 1234## 'data.frame': 200 obs. of 3 variables:## $ age : int 2 2 3 3 4 4 5 5 6 6 ...## $ sex : chr &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;male&quot; ...## $ expense: num 25.8 21 30 16.3 25.7 ... 12ggplot(leisure,aes(x=age,y=expense,col=sex))+ geom_line(size=1.2,linetype=3) #막대그래프의 순서변경-reorder() 1234567mpg1&lt;-read.csv(&quot;../data/mpg1.csv&quot;,stringsAsFactors = F)#데이터 가공drv_hwy&lt;-mpg1 %&gt;% group_by(drv) %&gt;% summarise(mean_hwy=mean(hwy))drv_hwy 123456## # A tibble: 3 × 2## drv mean_hwy## &lt;chr&gt; &lt;dbl&gt;## 1 4 19.2## 2 f 28.2## 3 r 21 123#기본 그래프ggplot(drv_hwy,aes(x=drv,y=mean_hwy))+ geom_col() 12ggplot(drv_hwy,aes(x=reorder(drv,mean_hwy),y=mean_hwy))+ geom_col() 123456789ggplot(drv_hwy,aes(x=reorder(drv,-mean_hwy),y=mean_hwy))+ geom_col()+ labs( title=&quot;그래프 제목을 입력하세요&quot;, subtitle=&quot;그래프 소제목을 입력하세요&quot;, x=&quot;x변수명을 입력하세요&quot;, y=&quot;y변수명을 입력하세요&quot;, caption=&quot;데이터 출처를 입력하세요&quot; )","link":"/2022/06/22/day0622/"},{"title":"day0623","text":"제목 1ㄴㄴㄴㄴ 소제목 1제목 2ㅁㅇㄹㅇㄹ 소제목 1-1##복습iris데이터,sepal.length,sepal.width 활용해서 종별로 산점도를 그리세요.-제목과 x축, y축을 변경하세요+x축 길이, y축 너비 12library(ggplot2)summary(iris) 1234567891011121314## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 12ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species))+ geom_point() 123library(ggplot2)str(iris) 123456## 'data.frame': 150 obs. of 5 variables:## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 123456789101112ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width, col=Species))+ geom_point()+ labs( title=&quot;제목&quot;, x=&quot;길이&quot;, y=&quot;너비&quot; )+ scale_color_manual( labels= c(&quot;setosa&quot;, &quot;vericolor&quot;,&quot;virginica&quot;), values= c(&quot;red&quot;,&quot;blue&quot;,&quot;yellow&quot;) )+ theme_classic() #평균차이 검정 -가설 검정: 평균의 차이를 검정 -남자의 평균키와 여자의 평균 키는 차이가 있을 것이다. #교차분석(=빈도분석)-가설검정: 각 범주별 비도를 활용해서 관계성을 검정#단순 회귀 분석 y =ax + b y는 종속변수, x가 정해져야지 정해진다. -온도 판매량 -가설 검정 기온(독립변수)이 판매량(종속변수)에 긍정적(부정적)영향을 주고 있을 것이다. 평균차이 분석, 회귀분석 ###가설-Hypothesis—-&gt; 공부 방법: 선행 연구, 해당분석 방법의 가설 설정 예시 존재-연구: 내가 궁금한 것을 찾는 것-귀무가설: 두 그룹간의 평균 차이가 없다.-대립가설: 두 그룹간의 평균 차이가 있다. 100명 중 5명 여기서 5명을 유의수준유의수준=&gt;0.05이내면 대립가설을 지지해주기로 했다.예외가 있다.=&gt; 예외가 있음을 인정하자 -가설 검정에서 인정하는 유의수준5%, 1%, 0.1% 또는 10% -A후보 44%-B후보 42% 이재명 vs 윤형선-이재명 t.test-어떻게 데이터를 입력하는지 확인-p.value, 유의수준 0.65이상: 귀무가설, 0.05이내 –&gt; 대립 가설 1234mpg1&lt;-read.csv(&quot;../data/mpg1.csv&quot;, stringsAsFactors =F)library(ggplot2)ggplot(mpg1,aes(x=trans,y=cty))+ geom_boxplot() -t.test검정+귀무가설: auto와 manual의 cty 평균은 차이가 없다. 대립 가설:auto와 manual의 cty 평균은 차이가 있다.(보다 작으면) 1t.test(data=mpg1, cty~trans) 1234567891011## ## Welch Two Sample t-test## ## data: cty by trans## t = -4.5375, df = 132.32, p-value = 1.263e-05## alternative hypothesis: true difference in means between group auto and group manual is not equal to 0## 95 percent confidence interval:## -3.887311 -1.527033## sample estimates:## mean in group auto mean in group manual ## 15.96815 18.67532 12#종속변수~독립변수#y(반응변수)x(설명변수) -두 그룹의 평균 차이 검정 +사전 필수 검증 +등분산 검정 +두 그룹간의 분산이 비슷하면 ==&gt; t.test(모수 검정) +두 그룹간의 분산이 다르면, –&gt;비모수 검정 +등분산 검정 +귀무가설: 두 그룹간의 분산이 비슷하다. +대립가설:ㅈㄹ 1var.test(data=mpg1,cty~trans) 1234567891011## ## F test to compare two variances## ## data: cty by trans## F = 0.73539, num df = 156, denom df = 76, p-value = 0.1101## alternative hypothesis: true ratio of variances is not equal to 1## 95 percent confidence interval:## 0.4912917 1.0719468## sample estimates:## ratio of variances ## 0.7353887 12ggplot(mpg1, aes(x=cty,fill=trans))+ geom_density(alpha=0.1) 교차분석-범주형 변수들이 관계가 있다는 것을 검정-비율에 차이가 있는지 검정-교차분석 검정은 R의chisq.test() 함수로 진행-귀무가설: trans에 따라 drv(4,f,r)의 (비율)차이가 없다.-대립가설: trans에 따라 drv의 차이가 있다. #빈도표 / 비율 1table(mpg1$trans, mpg1$drv) 1234## ## 4 f r## auto 75 65 17## manual 28 41 8 비율 1prop.table(table(mpg1$trans,mpg1$drv),1) 1234## ## 4 f r## auto 0.4777070 0.4140127 0.1082803## manual 0.3636364 0.5324675 0.1038961 -auto 4륜 구동(4)인 47.8%-manual 전륜구동(f) 53.2% 가장 많음-실제로 통계적으로 봤을 때, 차이가 있는지 검정 1chisq.test(mpg1$trans,mpg1$drv) 12345## ## Pearson's Chi-squared test## ## data: mpg1$trans and mpg1$drv## X-squared = 3.1368, df = 2, p-value = 0.2084 -차이가 없다.","link":"/2022/06/23/day0623/"},{"title":"day0624","text":"-통계 검정 +평균 차이 검정: 수치 데이터 +범주 데이터(두 그룹) -세 그룹 이상 +비율 차이 검정: 범주 데이터 +상관 관계: 수치 데이터 +회귀 -통계 검정 사전 준비 +분석을 위한 데이터가 적절한지 검정 +등분산 검정(두 평균 차이 검정을 하려고 할 때, 분산이 비슷한지 확인하는 것), 수치 데이터가 정규 분포를 이루는가(정규성 검정) -귀무가설, 대립가설 적절하게 설정 +서울의 평균 임금과 부산의 평균 임금이 차이가 있을 것이다. +선행 연구(논문을 찾아서 응용) -테스트 +t.test, chisq.test, cor.test +p.value -p.value&gt;0.05 –&gt; 귀무가설을 지지 -p.value&lt;0.05 –&gt; 대립가설을 지지 ##회귀의 중요성-기초 통계 : 특정한 결과에 영향을 주는 주 요인이 뭐냐? 이걸 찾는 것이 회귀.-회귀분석과 종류 +1세대 회귀 방법론: 다항 회귀분석, 다중 회귀분석, 포아송 회귀분석 +2세대 회귀 방법론: 구조 방정식 -귀무가설&amp;대립가설 존재 +귀무가설:x(=독립 변수)가 y(=종속 변수)에 영향을 주지 않는다. +대립가설:x(=독립 변수)가 y(=종속 변수)에 영향을 준다. -lm(종속 변수 ~ 독립 변수,data) +p.value 12RA&lt;-lm(data=mtcars,mpg~disp)summary(RA) 123456789101112131415161718## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 ***## disp -0.041215 0.004712 -8.747 9.38e-10 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 3.251 on 30 degrees of freedom## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 -머신 러닝, 인공지능 주 목적은 예측 y = ax + b ANOVA (분산 분석)식1: y=disp + var1 + var2 + var3식2: y=disp + var1 + var2 컬럼 개수가 100개-전진소거법, 후진소거법 R-squared : 결정계수= 설명력= 0~1-1로 수렴할수록 설명력이 좋다 특정한 결과에 영향을 주는 주 요인이 뭐냐? 이걸 찾는 것이 회귀 1lm(data=mtcars,mpg~disp) 1234567## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Coefficients:## (Intercept) disp ## 29.59985 -0.04122 1lm(formula=mpg~disp,data=mtcars) 1234567## ## Call:## lm(formula = mpg ~ disp, data = mtcars)## ## Coefficients:## (Intercept) disp ## 29.59985 -0.04122 #p-value가 0.5보다 작으면 회귀모형이 적합하다고 해석그렇지 않으면 회귀모형에 문제가 있는 것이므로 회귀분석 자체가 성립x 1lm(data= mtcars,mpg~disp+hp+wt) 1234567## ## Call:## lm(formula = mpg ~ disp + hp + wt, data = mtcars)## ## Coefficients:## (Intercept) disp hp wt ## 37.105505 -0.000937 -0.031157 -3.800891 12RA&lt;-lm(data=mtcars,mpg~disp+hp+wt)summary(RA) 1234567891011121314151617181920## ## Call:## lm(formula = mpg ~ disp + hp + wt, data = mtcars)## ## Residuals:## Min 1Q Median 3Q Max ## -3.891 -1.640 -0.172 1.061 5.861 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.105505 2.110815 17.579 &lt; 2e-16 ***## disp -0.000937 0.010350 -0.091 0.92851 ## hp -0.031157 0.011436 -2.724 0.01097 * ## wt -3.800891 1.066191 -3.565 0.00133 ** ## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 2.639 on 28 degrees of freedom## Multiple R-squared: 0.8268, Adjusted R-squared: 0.8083 ## F-statistic: 44.57 on 3 and 28 DF, p-value: 8.65e-11 1library(dplyr) 12## ## 다음의 패키지를 부착합니다: 'dplyr' 123## The following objects are masked from 'package:stats':## ## filter, lag 123## The following objects are masked from 'package:base':## ## intersect, setdiff, setequal, union 12library(ggplot2)library(foreign) 1mental&lt;-read.spss(&quot;../data/한국행정연구원_사회통합실태조사_데이터_2019.sav&quot;) 1## re-encoding from CP51949 1class(mental)#객체 유형 확인인 1## [1] &quot;list&quot; 12mental&lt;-as.data.frame(mental)#데이터프레임으로 변환하기기class(mental) 1## [1] &quot;data.frame&quot; 1str(mental)#mental 의 구조조 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101## 'data.frame': 8000 obs. of 276 variables:## $ id : num 1 2 3 4 5 6 7 8 9 10 ...## $ ara : Factor w/ 17 levels &quot;서울&quot;,&quot;부산&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...## $ wt1 : num 12216 12216 8455 7421 6474 ...## $ wt2 : num 2.58 2.58 1.79 1.57 1.37 ...## $ q1_1 : Factor w/ 12 levels &quot;0점 전혀 행복하지 않았다&quot;,..: 8 6 7 5 7 8 8 8 7 5 ...## $ q1_2 : Factor w/ 12 levels &quot;0점 전혀 하지 않았다&quot;,..: 5 5 5 4 4 4 6 6 4 6 ...## $ q1_3 : Factor w/ 12 levels &quot;0점 전혀 우울하지 않았다&quot;,..: 6 4 6 5 4 5 5 7 5 5 ...## $ q1_4 : Factor w/ 12 levels &quot;0점 전혀 만족하지 않는다&quot;,..: 5 5 5 4 7 5 6 6 5 5 ...## $ q1_5 : Factor w/ 12 levels &quot;0점 전혀 가치 없다&quot;,..: 6 4 5 4 8 7 7 6 5 4 ...## $ q2_1 : Factor w/ 11 levels &quot;0점 전혀 안전하지 않다&quot;,..: 5 6 7 5 7 8 6 7 6 6 ...## $ q2_2 : Factor w/ 11 levels &quot;0점 전혀 안전하지 않다&quot;,..: 4 7 6 4 7 7 7 5 4 5 ...## $ q2_3 : Factor w/ 11 levels &quot;0점 전혀 안전하지 않다&quot;,..: 4 7 6 5 7 5 6 4 4 6 ...## $ q3 : Factor w/ 11 levels &quot;0점 전혀 자유롭지 않다&quot;,..: 4 6 4 3 8 6 6 5 4 5 ...## $ q4 : Factor w/ 12 levels &quot;0점 매우 낮다&quot;,..: 4 6 5 4 6 7 6 4 4 6 ...## $ q5_1 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 3 3 2 3 3 3 2 2 2 2 ...## $ q5_2 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 2 3 3 2 2 3 3 2 1 2 ...## $ q5_2_1 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 2 3 2 3 2 3 2 2 2 2 ...## $ q5_2_2 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 2 4 2 2 2 2 2 2 1 2 ...## $ q5_3 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 3 3 3 3 3 3 2 2 1 2 ...## $ q5_4 : Factor w/ 5 levels &quot;전혀 그렇지 않다&quot;,..: 2 3 2 2 3 2 2 1 1 1 ...## $ q6 : Factor w/ 11 levels &quot;0점 전혀 만족하지 않는다&quot;,..: 7 7 6 6 7 8 7 5 7 4 ...## $ q7 : Factor w/ 11 levels &quot;0점 매우 나빠질 것이다&quot;,..: 7 6 6 5 6 7 8 6 5 4 ...## $ q8 : Factor w/ 11 levels &quot;0점 전혀 만족하지 않는다&quot;,..: 6 6 6 7 6 6 7 5 5 3 ...## $ q9 : Factor w/ 11 levels &quot;0점 매우 나빠질 것이다&quot;,..: 5 6 5 7 6 6 7 5 5 3 ...## $ q10 : Factor w/ 11 levels &quot;0점 전혀 만족하지 않는다&quot;,..: 6 9 7 8 7 9 7 6 5 6 ...## $ q11 : Factor w/ 11 levels &quot;0점 매우 나빠질 것이다&quot;,..: 4 9 6 5 5 7 6 6 5 8 ...## $ q12 : Factor w/ 11 levels &quot;북한은 우리의 적이다&quot;,..: 4 4 7 6 6 7 7 7 3 7 ...## $ q13 : Factor w/ 11 levels &quot;성장이 더 중요하다&quot;,..: 4 5 6 5 7 6 6 6 3 5 ...## $ q14 : Factor w/ 11 levels &quot;공동체의 이익이 더 중요하다&quot;,..: 4 4 6 4 7 7 7 6 3 5 ...## $ q15_1 : Factor w/ 4 levels &quot;고도의 경제성장&quot;,..: 1 1 1 1 3 2 2 2 4 3 ...## $ q15_2 : Factor w/ 4 levels &quot;고도의 경제성장&quot;,..: 2 3 4 4 2 1 1 4 1 4 ...## $ q16_1 : Factor w/ 4 levels &quot;국가의 질서 유지&quot;,..: 2 2 2 2 1 2 2 2 4 2 ...## $ q16_2 : Factor w/ 4 levels &quot;국가의 질서 유지&quot;,..: 3 4 4 3 4 3 3 4 1 3 ...## $ q17_1 : Factor w/ 4 levels &quot;경제 안정&quot;,&quot;범죄와의 전쟁&quot;,..: 2 3 3 2 2 1 2 2 4 1 ...## $ q17_2 : Factor w/ 4 levels &quot;경제 안정&quot;,&quot;범죄와의 전쟁&quot;,..: 4 2 1 3 3 4 3 3 2 4 ...## $ q18_1 : Factor w/ 4 levels &quot;전혀 동의하지 않는다&quot;,..: 2 2 2 3 2 3 3 2 3 2 ...## $ q18_2 : Factor w/ 4 levels &quot;전혀 동의하지 않는다&quot;,..: 2 3 3 2 3 2 3 3 2 3 ...## $ q18_3 : Factor w/ 4 levels &quot;전혀 동의하지 않는다&quot;,..: 2 2 3 2 2 2 3 2 2 2 ...## $ q19_1 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 2 1 1 1 1 1 1 3 1 ...## $ q19_2 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...## $ q19_3 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 1 1 3 1 1 1 1 1 1 ...## $ q19_4 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 3 1 1 3 2 3 3 3 3 1 ...## $ q19_5 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 2 1 1 1 2 1 1 1 1 ...## $ q19_6 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 3 1 1 1 1 1 2 1 1 1 ...## $ q19_7 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 3 1 1 1 3 1 2 3 2 1 ...## $ q19_8 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 1 2 1 2 1 1 1 1 1 ...## $ q19_9 : Factor w/ 6 levels &quot;소속된 적이 없다&quot;,..: 1 1 1 1 2 1 1 2 1 1 ...## $ q20_1 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 4 4 2 4 6 2 5 4 6 5 ...## $ q20_2 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 3 3 5 6 3 4 5 6 4 ...## $ q20_3 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 6 3 4 4 5 2 4 4 5 5 ...## $ q20_4 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 4 5 3 6 2 4 5 5 4 ...## $ q20_5 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 2 4 4 6 2 5 5 5 4 ...## $ q20_6 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 2 5 3 6 3 3 5 7 4 ...## $ q20_7 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 3 5 3 6 2 3 5 6 3 ...## $ q20_8 : Factor w/ 8 levels &quot;1점 전혀 중요하지 않다&quot;,..: 5 3 5 3 7 2 3 5 6 3 ...## $ q21_1 : Factor w/ 5 levels &quot;전혀 동의하지 않는다&quot;,..: 4 3 3 3 4 4 4 3 4 3 ...## $ q21_2 : Factor w/ 5 levels &quot;전혀 동의하지 않는다&quot;,..: 3 3 2 3 3 4 4 4 3 2 ...## $ q21_3 : Factor w/ 5 levels &quot;전혀 동의하지 않는다&quot;,..: 3 4 3 4 3 5 4 3 4 2 ...## $ q21_4 : Factor w/ 5 levels &quot;전혀 동의하지 않는다&quot;,..: 3 3 2 3 3 4 4 3 3 2 ...## $ q22_1 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 3 1 3 1 3 1 3 4 2 ...## $ q22_2 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 2 2 3 1 3 1 2 4 2 ...## $ q22_3 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 3 2 3 3 3 3 3 4 3 ...## $ q22_4 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 3 1 2 1 3 3 3 4 2 ...## $ q22_5 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 3 3 3 1 3 3 3 4 2 ...## $ q22_6 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 4 3 3 1 3 3 4 4 3 ...## $ q22_7 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 2 3 3 1 3 2 3 4 2 ...## $ q22_8 : Factor w/ 4 levels &quot;지난 1년 동안 한 적이 있다&quot;,..: 4 2 3 3 1 1 2 3 1 2 ...## $ q23 : Factor w/ 2 levels &quot;참여했다&quot;,&quot;참여하지 않았다&quot;: 1 1 1 1 1 1 2 1 1 1 ...## $ q24 : Factor w/ 2 levels &quot;참여했다&quot;,&quot;참여하지 않았다&quot;: 1 1 1 1 1 1 2 1 1 1 ...## $ q25 : Factor w/ 2 levels &quot;참여했다&quot;,&quot;참여하지 않았다&quot;: 1 1 1 2 1 1 2 1 1 1 ...## $ q26 : Factor w/ 2 levels &quot;있다&quot;,&quot;없다&quot;: 1 1 1 1 2 1 2 1 1 2 ...## $ q27 : Factor w/ 6 levels &quot;매우 보수적&quot;,..: 1 2 2 3 4 4 3 1 2 3 ...## $ q28_1 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 2 3 2 1 3 3 3 3 3 1 ...## $ q28_2 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 2 3 3 2 2 3 3 3 2 1 ...## $ q28_3 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 2 3 2 2 3 2 4 2 2 1 ...## $ q28_4 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 2 3 2 2 2 1 3 2 2 1 ...## $ q29_1 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 3 2 2 3 2 2 2 1 4 3 ...## $ q29_2 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 3 3 3 2 2 1 2 1 3 2 ...## $ q29_3 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 3 3 2 2 3 1 2 1 3 2 ...## $ q29_4 : Factor w/ 4 levels &quot;전혀 이루어지지 않고 있다&quot;,..: 3 3 2 2 3 1 2 1 3 2 ...## $ q30_1 : Factor w/ 5 levels &quot;없다&quot;,&quot;1~2명&quot;,..: 2 3 2 3 2 2 2 2 2 1 ...## $ q30_1_1 : Factor w/ 7 levels &quot;전화&quot;,&quot;우편&quot;,..: 4 4 4 4 4 4 4 4 4 NA ...## $ q30_1_2 : Factor w/ 6 levels &quot;이메일&quot;,&quot;SNS(페이스북, 트위터, 인스타그램 등)&quot;,..: NA NA NA NA NA NA NA NA NA NA ...## $ q30_2 : Factor w/ 7 levels &quot;없다&quot;,&quot;1~2명&quot;,..: 3 4 3 3 4 2 2 3 4 2 ...## $ q30_2_1 : Factor w/ 7 levels &quot;전화&quot;,&quot;우편&quot;,..: 1 4 4 4 3 4 1 4 1 5 ...## $ q30_2_2 : Factor w/ 6 levels &quot;이메일&quot;,&quot;SNS(페이스북, 트위터, 인스타그램 등)&quot;,..: NA NA NA NA 1 NA NA NA NA NA ...## $ q31_1 : Factor w/ 5 levels &quot;없다&quot;,&quot;1~2명&quot;,..: 4 3 1 3 3 2 1 4 3 3 ...## $ q31_2 : Factor w/ 5 levels &quot;없다&quot;,&quot;1~2명&quot;,..: 3 2 1 2 3 2 1 3 2 2 ...## $ q31_3 : Factor w/ 5 levels &quot;없다&quot;,&quot;1~2명&quot;,..: 4 3 1 2 3 2 1 4 2 2 ...## $ q32_1 : Factor w/ 4 levels &quot;전혀 그렇지 않다&quot;,..: 2 2 2 2 1 2 3 3 1 2 ...## $ q32_2 : Factor w/ 4 levels &quot;전혀 그렇지 않다&quot;,..: 2 2 2 1 1 2 3 3 1 2 ...## $ q32_3 : Factor w/ 4 levels &quot;전혀 그렇지 않다&quot;,..: 2 2 2 2 1 2 3 3 1 2 ...## $ q33 : Factor w/ 5 levels &quot;전혀 믿을 수 없다&quot;,..: 2 3 2 2 3 2 2 3 3 3 ...## $ q34_1 : Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 3 3 2 2 4 2 2 2 4 3 ...## $ q34_2 : Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 2 3 2 3 3 2 2 3 3 2 ...## $ q34_3 : Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 2 2 2 3 2 2 1 2 3 3 ...## $ q34_4 : Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 2 2 2 2 1 1 2 2 2 2 ...## $ q34_5 : Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 2 2 2 2 2 1 2 2 2 2 ...## $ q35_1 : Factor w/ 4 levels &quot;전혀 믿지 않는다&quot;,..: 3 3 2 2 2 2 1 2 3 3 ...## [list output truncated] 12345678910111213#추출 변수(변수 설명, 척도 범위) : 영어 변수 이름mental&lt;-mental %&gt;% select(q32_2,q1_4,q32_1,q34_1, q52,d17,,d1,d2,ara) %&gt;% rename(suicide=q32_2, satisfaction=q1_4, loneliness=q32_1, family_belief=q34_1, wealth=q52, health=d17, sex=d1, age=d2, area=ara)str(mental) 12345678910## 'data.frame': 8000 obs. of 9 variables:## $ suicide : Factor w/ 4 levels &quot;전혀 그렇지 않다&quot;,..: 2 2 2 1 1 2 3 3 1 2 ...## $ satisfaction : Factor w/ 12 levels &quot;0점 전혀 만족하지 않는다&quot;,..: 5 5 5 4 7 5 6 6 5 5 ...## $ loneliness : Factor w/ 4 levels &quot;전혀 그렇지 않다&quot;,..: 2 2 2 2 1 2 3 3 1 2 ...## $ family_belief: Factor w/ 4 levels &quot;전혀 신뢰하지 않는다&quot;,..: 3 3 2 2 4 2 2 2 4 3 ...## $ wealth : Factor w/ 11 levels &quot;0점 전혀 안정적이지 않다&quot;,..: 4 7 6 6 7 7 6 5 4 6 ...## $ health : Factor w/ 5 levels &quot;매우 나쁘다&quot;,..: 4 3 4 4 4 4 4 4 4 4 ...## $ sex : Factor w/ 2 levels &quot;남성&quot;,&quot;여성&quot;: 2 2 1 2 1 2 2 1 2 1 ...## $ age : Factor w/ 5 levels &quot;19~29세&quot;,&quot;30대&quot;,..: 5 5 4 4 3 3 1 5 5 3 ...## $ area : Factor w/ 17 levels &quot;서울&quot;,&quot;부산&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 12345678mental$suicide &lt;- as.integer(mental$suicide)mental$satisfaction&lt;- as.integer(mental$satisfaction)mental$loneliness&lt;-as.integer(mental$loneliness)mental$family_belief&lt;-as.integer(mental$family_belief)mental$wealth &lt;-as.integer(mental$wealth)mental$health&lt;-as.integer(mental$health)table(mental$suicide) 123## ## 1 2 3 4 ## 5592 1862 479 67 1table(mental$health) 123## ## 1 2 3 4 5 ## 87 509 2413 3730 1261 1table(mental$satisfaction) 123## ## 1 2 3 4 5 6 7 8 9 10 11 ## 49 79 170 302 440 2053 1611 1761 1040 321 174 #sex, age, area의 유형을 범주형에서 문자형으로 변환 12mental$satisfaction&lt;-mental$satisfaction-1mental$wealth&lt;-mental$wealth-1 #age 범주 이름 바꾸기 12mental$age&lt;-ifelse(mental$age==&quot;19~29세&quot;,&quot;20대&quot;, ifelse(mental$age==&quot;60~69&quot;,&quot;60대&quot;,mental$age)) #결측치, 이상치 확인하기 1summary(mental) 12345678910111213141516## suicide satisfaction loneliness family_belief ## Min. :1.000 Min. : 0.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.000 1st Qu.: 5.000 1st Qu.:1.000 1st Qu.:3.000 ## Median :1.000 Median : 6.000 Median :2.000 Median :4.000 ## Mean :1.378 Mean : 6.037 Mean :1.795 Mean :3.576 ## 3rd Qu.:2.000 3rd Qu.: 7.000 3rd Qu.:2.000 3rd Qu.:4.000 ## Max. :4.000 Max. :10.000 Max. :4.000 Max. :4.000 ## ## wealth health sex age area ## Min. : 0.000 Min. :1.000 남성:4011 Length:8000 경기 :1103 ## 1st Qu.: 4.000 1st Qu.:3.000 여성:3989 Class :character 서울 : 965 ## Median : 5.000 Median :4.000 Mode :character 부산 : 539 ## Mean : 4.985 Mean :3.696 경남 : 527 ## 3rd Qu.: 6.000 3rd Qu.:4.000 인천 : 522 ## Max. :10.000 Max. :5.000 경북 : 466 ## (Other):3878 #성별 빈도분석 12345mental %&gt;% group_by(sex) %&gt;% summarise(n=n()) %&gt;% #sex 변수의 범주별 빈도 구하기 mutate(total=sum(n), pct=round(n/total*100,1)) 12345## # A tibble: 2 × 4## sex n total pct## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;## 1 남성 4011 8000 50.1## 2 여성 3989 8000 49.9 #연령대별 빈도분석 12345mental %&gt;% group_by(age) %&gt;% summarise(n=n()) %&gt;% mutate(total=sum(n), pct=round(n/total*100,1)) 12345678## # A tibble: 5 × 4## age n total pct## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;## 1 2 1516 8000 19 ## 2 20대 1542 8000 19.3## 3 3 1769 8000 22.1## 4 4 1821 8000 22.8## 5 5 1352 8000 16.9 #성과 연령대의 교차 빈도 구하기 1table(mental$sex,mental$age) 1234## ## 2 20대 3 4 5## 남성 745 822 900 891 653## 여성 771 720 869 930 699 #성과 연령대의 교차 백분율 구하기, 행별로 100%기준.소수점 한자리 1round(prop.table(table(mental$sex,mental$age),1)*100,1) 1234## ## 2 20대 3 4 5## 남성 18.6 20.5 22.4 22.2 16.3## 여성 19.3 18.0 21.8 23.3 17.5 #교차분석 검정 1chisq.test(mental$sex,mental$age) 12345## ## Pearson's Chi-squared test## ## data: mental$sex and mental$age## X-squared = 10.076, df = 4, p-value = 0.03916 #p&lt;0.05이므로 연령대 분포 비율은 다소 차이가 있다. #suicide, satisfaction, loneliness, family_belif, wealth,health 변수의 평균을 분석한다. 12mental %&gt;% summarise(m1=mean(suicide), m2=mean(satisfaction),m3=mean(loneliness),m4=mean(family_belief),m5=mean(wealth),m6=mean(health)) 12## m1 m2 m3 m4 m5 m6## 1 1.377625 6.0365 1.795 3.576375 4.985125 3.696125 #자살충동은 4점 척도에서 1.38 따라서 낮은 편#삶의 만족도 6.04 보통보다 조금 위#외로움 1.8으로 상대적으로 낮은 편#가족신뢰도 높은편#경제 안정도: 보통 바로 밑#건강상태: 5점 척도에서 3.7으로 좋은 수준 #삶의 만족도와 외로움을 독립변수로 하고, 자살충동을 종속변수로 하는 다중회귀분석 12RA&lt;-lm(data=mental,suicide~satisfaction+loneliness)summary(RA) 12345678910111213141516171819## ## Call:## lm(formula = suicide ~ satisfaction + loneliness, data = mental)## ## Residuals:## Min 1Q Median 3Q Max ## -1.50517 -0.40228 -0.03487 0.17773 3.07029 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.035551 0.029823 34.72 &lt;2e-16 ***## satisfaction -0.052583 0.003614 -14.55 &lt;2e-16 ***## loneliness 0.367405 0.007987 46.00 &lt;2e-16 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 0.5451 on 7997 degrees of freedom## Multiple R-squared: 0.2668, Adjusted R-squared: 0.2666 ## F-statistic: 1455 on 2 and 7997 DF, p-value: &lt; 2.2e-16 #suicide = 1.035551-0.052583 X satisfaction + 0.367405 X loneliness #삶의 만족도와 외로움의 상관관계 1cor.test(mental$satisfaction,mental$loneliness) 1234567891011## ## Pearson's product-moment correlation## ## data: mental$satisfaction and mental$loneliness## t = -25.374, df = 7998, p-value &lt; 2.2e-16## alternative hypothesis: true correlation is not equal to 0## 95 percent confidence interval:## -0.2931116 -0.2525481## sample estimates:## cor ## -0.2729512 #가족 신뢰도, 경제안정도, 건강상태가 삶의 만족도와 외로움에 미치는 옇야 12RA&lt;-lm(data=mental, satisfaction~family_belief+wealth+health)summary(RA) 123456789101112131415161718192021## ## Call:## lm(formula = satisfaction ~ family_belief + wealth + health, ## data = mental)## ## Residuals:## Min 1Q Median 3Q Max ## -6.8274 -0.9431 -0.0425 1.0569 6.1986 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.07613 0.13765 15.08 &lt;2e-16 ***## family_belief 0.36851 0.03196 11.53 &lt;2e-16 ***## wealth 0.26016 0.01089 23.88 &lt;2e-16 ***## health 0.36403 0.02206 16.50 &lt;2e-16 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 1.627 on 7996 degrees of freedom## Multiple R-squared: 0.1386, Adjusted R-squared: 0.1383 ## F-statistic: 428.8 on 3 and 7996 DF, p-value: &lt; 2.2e-16 #3개의 독립 변수가 외로움에 미치는 영향 12RA&lt;-lm(data=mental, loneliness~family_belief+wealth+health)summary(RA) 1234567891011121314151617181920## ## Call:## lm(formula = loneliness ~ family_belief + wealth + health, data = mental)## ## Residuals:## Min 1Q Median 3Q Max ## -2.24066 -0.64247 0.01863 0.43022 2.83959 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.652247 0.063109 57.87 &lt;2e-16 ***## family_belief -0.220274 0.014654 -15.03 &lt;2e-16 ***## wealth -0.072686 0.004995 -14.55 &lt;2e-16 ***## health -0.191313 0.010116 -18.91 &lt;2e-16 ***## ---## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 0.746 on 7996 degrees of freedom## Multiple R-squared: 0.1157, Adjusted R-squared: 0.1154 ## F-statistic: 348.9 on 3 and 7996 DF, p-value: &lt; 2.2e-16 =&gt;3개의 변수 모두 유의미하다.loneliness = 3.652247 -0.220274 X family_belief - 0.072686 X wealth -0.191313 X health 성별로 삶의 만족도에서 차이가 있는가를 독립표본 t검정으로 확인1t.test(data=mental,satisfaction~sex) 1234567891011## ## Welch Two Sample t-test## ## data: satisfaction by sex## t = -3.7719, df = 7997.6, p-value = 0.0001632## alternative hypothesis: true difference in means between group 남성 and group 여성 is not equal to 0## 95 percent confidence interval:## -0.22446298 -0.07094075## sample estimates:## mean in group 남성 mean in group 여성 ## 5.962852 6.110554 #유의수준이 0.001보다 작기 때문에 이 통계 결과는 유의미하다.여성의 만족도가 남성보다 높다고 할 수 있다. #연령대별 삶의 만족도 차이 1234mental %&gt;% group_by(age) %&gt;% summarise(m=mean(satisfaction)) %&gt;% arrange(desc(m)) 12345678## # A tibble: 5 × 2## age m## &lt;chr&gt; &lt;dbl&gt;## 1 2 6.13## 2 4 6.08## 3 3 6.05## 4 20대 6.04## 5 5 5.84 #30대가 6.13으로 가장 높다. #지역별 삶의 만족도 분석과 그래프 그리기 1234area_satisfaction&lt;-mental %&gt;% group_by(area) %&gt;% summarise(m=mean(satisfaction)) %&gt;% arrange(desc(m)) 123456ggplot(data=area_satisfaction,aes(x=reorder(area,m),y=m))+ geom_col()+ ggtitle(&quot;지역별 만족도&quot;)+ xlab(&quot;지역&quot;)+ ylab(&quot;만족도&quot;)+ coord_flip()","link":"/2022/06/24/day0624/"},{"title":"","text":"1 ##제목1-파이썬 입문 ###소제목 1-여기는 소제목입니다. 1print(&quot;Hello World&quot;) Hello World 제목2-제목2섹션 ###소제목2 12a1=10print(a1) 10 ##제목3-여기는 제목3입니다. 1print(&quot;hellol&quot;) hellol 1","link":"/2022/06/27/basic0627/"},{"title":"","text":"title:”Python 기초문법”date:’2022-06-27’ 1 -Hello World 1print(&quot;Hello World&quot;) Hello World ##주석 처리-1줄 주석 -여러 줄 주석 처리 함수 또는 클래스를 문서화 할 때 주로 사용 -프로젝트 할 때 전체 공정 100 코드 / 코드 문서화/ 한글작업 문서화 123456#print() 함수 사용# print(&quot;1줄 주석&quot;)&quot;&quot;&quot;print(&quot;hello&quot;)&quot;&quot;&quot;print(&quot;hello&quot;) hello 변수-자료형 -스칼라형,non스칼라형 수치형 자료형 int,float 123num_int =1print(num_int)print(type(num_int)) 1 &lt;class 'int'&gt; 123num_float=0.1print(num_float)print(type(num_float)) 0.1 &lt;class 'float'&gt; Bool형-True,False -R: TRUE,FALSE 123bool_true =Trueprint(bool_true)print(type(bool_true)) True &lt;class 'bool'&gt; None 자료형-Null값, 값이 정해지지 않은 자료형 123none_x=Noneprint(none_x)print(type(none_x)) None &lt;class 'NoneType'&gt; 사칙연산정수형 사칙연산, 실수형 사칙연산 정수형 사칙연산, 실수형 사칙연산 결과값의 자료형 +,-,*,/ 123456a=3b=2print('a + b = ',a+b)print('a - b = ',a-b)print('a * b = ',a*b)print('a / b = ',a/b) a + b = 5 a - b = 1 a * b = 6 a / b = 1.5 123456a=1.5b=2.5print('a + b = ',a+b)print('a - b = ',a-b)print('a * b = ',a*b)print('a / b = ',a/b) a + b = 4.0 a - b = -1.0 a * b = 3.75 a / b = 0.6 ##논리형 연산자 Bool형 True와 False 값으로 정의 조건식 *교집합(and), 합집합(or) 12345678print(True and True)print(True and False)print(False and True)print(False and False)print(True or True)print(True or False)print(False or True)print(False or False) True False False False True True True False ##비교 연산자 비교 연산자는 부등호를 의미한다. 12345print(4&gt;3)print(4&lt;3)a=4&gt;3print(a)print(4&gt;3 and 3&gt;4) True False True False 논리형 &amp;비교 연산자 응용 input() 형변환 데이터 타입을 바꾸는 것 123var=int (input(&quot;숫자를 입력하세요&quot;))print(var)print(type(var)) 숫자를 입력하세요1 1 &lt;class 'int'&gt; 123456num1= int(input(&quot;첫번째 숫자를 입력하세요&quot;))num2= int(input(&quot;두번째 숫자를 입력하세요&quot;))num3= int(input(&quot;세번째 숫자를 입력하세요&quot;))num4= int(input(&quot;네번째 숫자를 입력하세요&quot;)) 첫번째 숫자를 입력하세요100 두번째 숫자를 입력하세요200 세번째 숫자를 입력하세요12 네번째 숫자를 입력하세요150 12var1=num1&gt;=num2var2=num3&lt;num4 1print(var1 and var2) False ##Stirng Non Scalar 1234print('Hello World')print(&quot;Hello World&quot;)print('&quot;Hello World&quot;')print(&quot;'Hello World'&quot;) Hello World Hello World &quot;Hello World&quot; String Operators 문자열 연산자 +,* 가능 1234str1=&quot;Hello &quot;str2=&quot;World&quot;print(str1 + str2) Hello World 12greet = str1 + str2print(greet *2) Hello WorldHello World 12multiline = &quot;Life is too short\\nYou need python&quot;print(multiline) Life is too short You need python 123head =&quot;Python&quot;tail=&quot; is fun!&quot;head+tail 'Python is fun!' 12a=&quot;Life is too short, You need Python&quot;a[0:4] 'Life' 123456a=&quot;20220627Rainy&quot;date=a[:8]weather= a[8:]dateweather 'Rainy' 12number=3&quot;I eat {} apples&quot;.format(number) 'I eat 3 apples' 1234nubmer=10day=&quot;three &quot;&quot;I ate {} apples. So I was sick for {}days&quot;.format(number,day) 'I ate 3 apples. So I was sick for three dyas' 1&quot;I ate {0} apples. so I was sick for {day} days&quot;. format(10,day=3) 'I ate 10 apples. so I was sick for 3 days' ##문자 개수 세기 12a=&quot;hobby&quot;a.count('b') 2 위치 알려주기12a=&quot;Life is too short&quot;a.index('t') 8 , 삽입하기1&quot;,&quot;.join('abcd') 'a,b,c,d' 12a=&quot;Life is too short&quot;a.replace(&quot;Life&quot;,&quot;Your leg&quot;) 'Your leg is too short' 1234a=[1,2,3,['a','b','c']]a[0]a[3]a[3][0] 'a' 1234a=[1,2,3]b=[4,5,6]a+b [1, 2, 3, 4, 5, 6] 12a=[1,2,3]len(a) 3 123a=[1,2,3,4,5]del a[2:]a [1, 2] 123a=[1,2,3]a.append(4)a [1, 2, 3, 4] 123a=[1,2,3]a.append([5,6])a [1, 2, 3, [5, 6]] 1234a=[1,2,3]a.insert(0,4)a [4, 1, 2, 3] 123t1=(1,2,'a','b')t1[0]t1[3] 'b' 123t1=(1,2,'a','b')t2=(3,4)t1+t2 (1, 2, 'a', 'b', 3, 4) 123a={'name':'김승욱','phone':'01096270326','birth':'0323'}a.get('name')a.get('phone') '01096270326' 12s1=set([1,2,3])s1 {1, 2, 3} 12s2=set(&quot;Hello&quot;)s2 {'H', 'e', 'l', 'o'} 123s1=set([1,2,3])l1=list(s1)l1 [1, 2, 3] 12a=[1,2,3]id(a) 140084781446080 ##Q1 1234567a=80b=75c=55add=80+75+55mean=add/3mean 70.0 ##Q2 12345a=13if(a%2==0): print(&quot;짝수다&quot;)else: print(&quot;홀수다&quot;) 홀수다 ##Q3 123a=&quot;881120-1068234&quot;print(&quot;19&quot;+a[:2])print(a[2:6]) 1988 1120 ##Q4 123a=&quot;a:b:c:d&quot;a.replace(&quot;:&quot;,&quot;#&quot;) 'a#b#c#d' ##Q5 123456pin=&quot;881120-1068234&quot;if(pin[7]=='1'): print(&quot;남자다&quot;)else: print(&quot;여자다&quot;) 남자다 ##Q6 1234a=[1,3,5,4,2]a.sort()a.reverse()a [5, 4, 3, 2, 1] ##Q7 12a=['Life', 'is', 'too','short']&quot; &quot;.join(a) 'Life is too short' ##Q8 1234a=[1,2,3]a.insert(0,4)a [4, 1, 2, 3] ##슬라이싱 12greeting=&quot;Hello Kaggle&quot;print(greeting[:5]) Hello ##Q9 1234a=dict()b=dict()c=dict()d=dict() 12a['name']='python'a {'name': 'python'} 12b[('b,')]='python'b {'b,': 'python'} 12d[250]='python'd {250: 'python'} ##Q10 123a={'A':90, 'B':80,'C':70}aa['B'] 80 ##Q11 1234a=[1,1,1,2,2,3,3,3,4,4,5]a1=set(a)a2=list(a1)a2 [1, 2, 3, 4, 5] ##Q12 123a=b=[1,2,3]a[1]=4print(b) [1, 4, 3] 12345from copy import copya=[1,2,3]b=copy(a)a[1]=4b [1, 2, 3] if문12345pocket = ['paper','cellphone', 'money']if 'money' in pocket: print(&quot;택시 타라&quot;)else: print(&quot;걸어가라&quot;) 택시 타라 if elif else12345678pocket=['paper','cellphone']card=bool_trueif 'money' in pocket: print(&quot;택시 타라&quot;)elif card: print(&quot; 택시 타라&quot;)else: print(&quot;걸어가라&quot;) 택시 타라 ##while 1234567treehit=0while treehit &lt; 10: treehit= treehit +1 print(&quot;나무를 %d 번 찍었습니다.&quot;%treehit) if treehit ==10: print(&quot;나무가 넘어간다&quot;) 나무를 1 번 찍었습니다. 나무를 2 번 찍었습니다. 나무를 3 번 찍었습니다. 나무를 4 번 찍었습니다. 나무를 5 번 찍었습니다. 나무를 6 번 찍었습니다. 나무를 7 번 찍었습니다. 나무를 8 번 찍었습니다. 나무를 9 번 찍었습니다. 나무를 10 번 찍었습니다. 나무가 넘어간다 12345678910prompt=&quot;&quot;&quot; 1. add 2. del 3. list 4. quit enter number:&quot;&quot;&quot; 1234number=0while number!=4: print(prompt) number = int(input()) 1. add 2. del 3. list 4. quit enter number: 4 While문 강제로 빠져나가기123456789101112131415coffee =10while True: money = int(input(&quot;돈을 넣어라&quot;)) if money ==300: print(&quot;커피를 줍니다.&quot;) coffee = coffee -1 elif money&gt;300: print(&quot;거스름돈 %d를 주고 커피를 줍니다.&quot;%(money -300)) coffee-=1 else: print(&quot;안줘 ㅎ&quot;) if coffee==0: print(&quot;커피 없다 돌아가라&quot;) break 돈을 넣어라500 거스름돈 200를 주고 커피를 줍니다. 돈을 넣어라100 안줘 ㅎ 돈을 넣어라300 커피를 줍니다. 돈을 넣어라300 커피를 줍니다. 돈을 넣어라300 커피를 줍니다. 돈을 넣어라300 커피를 줍니다. 돈을 넣어라300 커피를 줍니다. 돈을 넣어라300 커피를 줍니다. 돈을 넣어라540 거스름돈 240를 주고 커피를 줍니다. 돈을 넣어라320 거스름돈 20를 주고 커피를 줍니다. 돈을 넣어라532 거스름돈 232를 주고 커피를 줍니다. 커피 없다 돌아가라 12345678a=[1,2,3,4,5]rem = a.pop(1)print(a)print(rem)rem=a.pop()print(a)print(rem) [1, 3, 4, 5] 2 [1, 3, 4] 5 clear(): 리스트 내 모든 값 삭제 index(“값”) : 값의 위치를 불러옴 1234a=[1,2,3,4,5]b=[&quot;철수&quot;,&quot;영희&quot;,&quot;길동&quot;]print(a.index(4))print(b.index(&quot;길동&quot;)) 3 2 while문의 맨 처음으로 돌아가기12345678a=0b=100while a&lt;10: a=a+1 if a%2==0: continue print(a) print(b) 1 100 3 100 5 100 7 100 9 100 12345678a=0b=100while a&lt;10: a=a+1 if a%2==0: break; print(a) 1 123test_list = ['one', 'two','three']for i in test_list: print(i) one two three ##튜플 1234567tuple1=(0)tuple2=(0,)tuple3=0,1,2print(type(tuple1))print(type(tuple2))print(type(tuple3)) &lt;class 'int'&gt; &lt;class 'tuple'&gt; &lt;class 'tuple'&gt; 123a=[(1,2),(3,4),(5,6)]for(first,last) in a: print(first+last) 3 7 11 123456789marks=[90,25,67,45,80]number=0for mark in marks: number+=1 if mark&gt;60: print(&quot;%d번째 학생은 합격이다&quot; % number) else: print(&quot;넌 뭐했냐 %d번째야&quot; % number) 1번째 학생은 합격이다 넌 뭐했냐 2번째야 3번째 학생은 합격이다 넌 뭐했냐 4번째야 5번째 학생은 합격이다 for문과 continue12345678marks =[90,25,67,45,80]number = 0for mark in marks: number+=1 if mark &lt;60: continue print(&quot;%d 학생 합격입니다.&quot; %number) 1 학생 합격입니다. 3 학생 합격입니다. 5 학생 합격입니다. 123456789temp_dict={ 'teacher':'evan', 'class': 15, 'student': ['s1','s2','s3']}print(temp_dict[&quot;teacher&quot;])print(temp_dict['class'])print(temp_dict[&quot;student&quot;]) evan 15 ['s1', 's2', 's3'] 1list(temp_dict.keys()) ['teacher', 'class', 'student'] items() key -value 쌍으로, list와 tuple 형태로 변환 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-179-7c69bfb1b247&gt; in &lt;module&gt;() ----&gt; 1 dict_items([('teacher','evan'),('class', 15),]) NameError: name 'dict_items' is not defined ##for 문과 함께 자주 사용되는 range 함수 12a = range(10)a range(0, 10) 12a=range(1,11)a range(1, 11) 1234add=0for i in range(1,11): add=add+iprint(add) 55 12345marks=[90,25,67,45,80]for number in range(len(marks)): if marks[number]&lt;60: continue print(&quot;%d번째 학생 축하하눙&quot; % (number+1)) 1번째 학생 축하하눙 3번째 학생 축하하눙 5번째 학생 축하하눙 for 와 range 함수를 이용한 구구단1234for i in range(2,10): for j in range(2,10): print(i * j, end=&quot; &quot;) print(&quot; &quot;) 4 6 8 10 12 14 16 18 6 9 12 15 18 21 24 27 8 12 16 20 24 28 32 36 10 15 20 25 30 35 40 45 12 18 24 30 36 42 48 54 14 21 28 35 42 49 56 63 16 24 32 40 48 56 64 72 18 27 36 45 54 63 72 81 12345a=[1,2,3,4]result=[]for num in a: result.append(num*3)print(result) [3, 6, 9, 12] Q1 123456789101112a=&quot;Life is too short, you need python&quot;if &quot;wife&quot; in a: print(&quot;wife&quot;)elif &quot;python&quot; in a and &quot;you&quot; not in a: print(&quot;python&quot;)elif &quot;shirt&quot; not in a: print(&quot;shirt&quot;)elif &quot;need&quot; in a: print(&quot;need&quot;)else: print(&quot;nono&quot;) shirt Q2 while 문을 사용해 1부터 1000까지의 자연수 중 3의 배수의 합을 구해라 12345sum=0for i in range(1,1000): if i%3==0: sum+=iprint(sum) 166833 Q3 while 문을 사용하여 다음과 같이 별(*)을 표시하는 프로그램을 작성해 보 12345for i in range(0,5): for j in range(0,5): if j&lt;=i: print(&quot;*&quot;,end=&quot;&quot;) print(&quot; &quot;) * ** *** **** ***** ##Q3-1 7X7모양에서 다이아몬드 모양을 만들어봐라 1234for i in range(5): print(' '*(4-i)+'*'*(2*i-1))for i in range(4): print(' '+' '*i+'*'*(5-2*i)) * *** ***** ******* ***** *** * ##Q4 for 문을 사용해 1부터 100까지의 숫자를 출력해봐 12for i in range(1,101): print(i) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ##Q5 A학급에 총 10명의 학생이 있다. 이 학생들의 중간고사 점수는 다음과 같다.[70,60,55,75,95,90,80,80,85,100]for 문을 사용하여 A학급의 평균 점수를 구해 123456sum=0a=[70,60,55,75,95,90,80,80,85,100]for i in range(len(a)): sum+=a[i]sum/len(a) 79.0 ##Q6리스트 중에서 홀수에만 2를 곱하여 저장하는 다음 코드가 있다.numbers = [1, 2, 3, 4, 5]result = []for n in numbers: if n % 2 == 1: result.append(n*2)위 코드를 리스트 내포(list comprehension)를 사용하여 표현해 봐라 123numbers = [1, 2, 3, 4, 5]result = [n*2 for n in numbers if n%2==1]print(result) [2, 6, 10] 12345a=&quot;Kaggle&quot;for x in a: if x=='g': break; print(x) K a 123alphabets=['A','B','C']for index, value in enumerate(alphabets): print(index,value) 0 A 1 B 2 C","link":"/2022/06/27/day0627/"},{"title":"","text":"반복문 복습 for loop and while loop 123456a = &quot;Kaggle&quot;for i in a: if i==&quot;a&quot;: break; 리스트 1234567numbers = [1,2,3,4,5]sum=0for num in numbers: sum+=numprint(sum) 15 12345678fruits=['apple','kiwi', 'mango']newlist =[]for fruit in fruits: if &quot;a&quot; in fruit: newlist.append(fruit)print(newlist) ['apple', 'mango'] 12345a=[1,2,3]b=ab[1]=9print(a)print(b) [1, 9, 3] [1, 9, 3] 사용자 정의 함수 내가 필요에 의해 직접 함수를 작성 123def 함수명(param1, param2): #코드 return None 12345678910111213141516def add(a=0,b=1): &quot;&quot;&quot;a,b를 더하는 함수 Parameters: a(int): int형 숫자 a입력 b(int): int형 숫자 b입력 return: int:반환값 &quot;&quot;&quot; c=a+b return cprint(add(5,4))print(add.__doc__) 9 a,b를 더하는 함수 Parameters: a(int): int형 숫자 a입력 b(int): int형 숫자 b입력 return: int:반환값 1234567891011121314151617181920212223def add(a,b): c=a+b return cdef minus(a,b): c=a-b return cdef multi(a,b): c=a*b return cdef divide(a,b): c=a/b return cprint(add(4,5))print(minus(4,5))print(multi(4,5))print(divide(4,5)) 9 -1 20 0.8 여러개의 변수를 받을 때123456789def add_many(*args): result = 0 for i in args: result = result +i print(type(result)) return result add_many(1,2,3,4,5,5,6,8,89) &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; &lt;class 'int'&gt; 123 1234567a=1def vartest(a): a+=1 return aa=vartest(a)print(a) 2 1234567a=1def vartest(): global a a = a+1vartest()print(a) 2 함수 문서화 키워드: Docstinrg 123456789class Calculator: def __init__self(self): self.result =0 def add(self, num): self.result += num return self. result 123456789import numpy as npA=[1,2,3]B=[4,5,6]np_A = np.array(A)np_B = np.array(B)np_A / np_B ** 2 array([0.0625 , 0.08 , 0.08333333]) NumPy 내장모듈(=라이브러리 = 패키지)(X) 별도 라이브러리 설치 12import numpy as npprint(np.__version__) 1.21.6 12345temp=[1,2,3]temp_array = np.array(temp)print(type(temp))print(type(temp_array)) # 배열로 변환이 되었다는 것을 의미한다. &lt;class 'list'&gt; &lt;class 'numpy.ndarray'&gt; 123456789math_score=[90,80,100]eng_score=[80,90,100]print(math_score + eng_score)np_math = np.array(math_score)np_eng = np.array(eng_score)total = np_math + np_engprint(total)print(type(total)) [90, 80, 100, 80, 90, 100] [170 170 200] &lt;class 'numpy.ndarray'&gt; 집계 함수123print(np.min(total))print(np.max(total))print(np.sum(total)) 170 200 540 차원 배열의 차원 확인 필요 1234# 1차원 배열temp_arr = np.array([1,2,3])print(temp_arr.shape) #값이 3개print(temp_arr.ndim) #1차원 (3,) 1 12345# 2차원 배열temp_arr=np.array([[1,2,3],[4,5,6]])print(temp_arr.shape) #2*3배열print(temp_arr.ndim) #2차원print(temp_arr) (2, 3) 2 [[1 2 3] [4 5 6]] 12345# 3차원 배열 -&gt; 이미지temp_arr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])print(temp_arr.shape)print(temp_arr.ndim)print(temp_arr) (2, 2, 3) 3 [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]] 배열 생성의 다양한 방법들 모두 0으로 채운다 12import numpy as npprint(np.__version__) 1.21.6 12temp_arr= np.zeros((2,3))temp_arr array([[0., 0., 0.], [0., 0., 0.]]) 모두 1로 채운다 12temp_arr = np.ones((2,3))temp_arr array([[1., 1., 1.], [1., 1., 1.]]) 임의의 상수값으로 채운다 12temp_arr = np.full((3,3),5)temp_arr array([[5, 5, 5], [5, 5, 5], [5, 5, 5]]) 최소, 최대 숫자의 범위를 정해두고, 각구간별로 값을 생성 12temp_arr = np.linspace(5,10,10)temp_arr array([ 5. , 5.55555556, 6.11111111, 6.66666667, 7.22222222, 7.77777778, 8.33333333, 8.88888889, 9.44444444, 10. ]) 반복문 시 , 자주 등장하는 배 12temp_arr = np.arange(1,11,1)temp_arr array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 난수 생성123from numpy import randomx=random.rand()print(x) 0.4197892844762233 123import numpyx= numpy.random.rand()print(x) 0.8489952461394101 랜덤 정수값 추123from numpy import randomx= random.randint(100, size=(90))print(x) [65 20 95 55 13 39 19 56 8 26 44 23 59 6 32 97 87 18 42 55 28 98 2 69 68 8 58 20 8 76 4 68 6 97 42 83 34 92 10 16 59 79 57 16 41 75 65 51 99 54 88 17 94 50 1 83 37 64 78 72 50 93 64 84 36 40 98 99 11 66 39 54 43 65 38 31 98 36 35 71 78 11 1 99 57 87 47 50 25 53] 랜덤 배열 ,실수1234from numpy import randomx = random.rand(2,5)print(x)type(x) [[0.10636109 0.39654714 0.98230141 0.68008412 0.45948323] [0.02152314 0.53716145 0.87837662 0.02882575 0.8048422 ]] numpy.ndarray numpy 사칙 연1234import numpy as nparray_01 = np.array([1,2,3])array_02 = np.array([10,20,30]) 123456789101112newArr = np.add(array_01,array_02)print(newArr)newArr = np.subtract(array_01,array_02)print(newArr)newArr = np.multiply(array_01,array_02)print(newArr)newArr = np.divide(array_01,array_02)print(newArr)array_01 = np.array([1,2,3])array_02 = np.array([2,2,2])newArr = np.power(array_01,array_02)print(newArr) [3 4 5] [-1 0 1] [2 4 6] [0.5 1. 1.5] [1 4 9] ##소수점 정렬 소수점을 정렬하는 다양한 방법 123456import numpy as nptemp_arr = np.trunc([-1,23,123])print(temp_arr)temp_arr = np.fix([-1.23,1.23])print(temp_arr) [ -1. 23. 123.] [-1. 1.] 12temp_arr = np.floor([-1.2124,1.24242])print(temp_arr) [-2. 1.] 12temp_arr = np.ceil([-1.23123,1.23123])print(temp_arr) [-1. 2.] 조건식 pandas numpy 조건식 하나의 조건식 다중 조건 12temp_arr = np.arange(10)temp_arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12345# 5보다 작으면 원 값 유지# 5보다 크면 곱하기 10을 해주기# np.where(조건식, 참일 때, 거짓일 때)np.where (temp_arr&lt;5, temp_arr, temp_arr * 10) array([ 0, 1, 2, 3, 4, 50, 60, 70, 80, 90]) 12345temp_arr = np.arange(10)cond_list = [temp_arr &gt; 5, temp_arr &lt; 2]choice_list = [temp_arr *2 , temp_arr +100]# np.select(조건식 리스트, 결과값 리스트, default)np.select(cond_list,choice_list,default=temp_arr) array([100, 101, 2, 3, 4, 5, 12, 14, 16, 18]) Reshape 배열의 차원 또는 크기를 바꾼다. 곱셈 1234import numpy as nptemp_array = np.ones((3,4))print(temp_array.shape)print(temp_array) (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] 1234after_reshape = temp_array.reshape(2,-1) #-1을 넣으면 알아서 계산해준다# -1넣으면 알아서 계산해준다print(after_reshape.shape)print(after_reshape) (2, 6) [[1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1.]] def 함수이름(매개변수): return 반환값1, 반환값2 123456def add_sub(a,b): return a+b,a-bx,y=add_sub(20,15)print(x)print(y) 35 5 12345678def one_two(): return [1,2]x,y=one_two()print(x)print(y)print(type(x))print(type(y)) 1 2 &lt;class 'int'&gt; &lt;class 'int'&gt; 1234567x=10y=3def get_quotient_remainder(x,y): return x//y,x%yquotient, remainder = get_quotient_remainder(x,y)print('몫 : {0}, 나머지: {1}'.format(quotient, remainder)) 몫 : 3, 나머지: 1 pandas 튜토리12import pandas as pdprint(pd.__version__) 1.3.5 12345678temp_dict = { 'col1': [1,2], 'col2': [3,4]}df= pd.DataFrame(temp_dict)print(df)print(type(df)) col1 col2 0 1 3 1 2 4 &lt;class 'pandas.core.frame.DataFrame'&gt; 12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive 12345DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/human_ai/Basic/Chapter 3. pandas/data/'lemonade = pd.read_csv(DATA_PATH +'Lemonade2016.csv')# covid_df = pd.read_csv(DATA_PATH +)lemonade.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 1234import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 12s = pd.Series([1,3,5, np.nan,6,8])s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 12dates = pd.date_range('20220627', periods=6)dates DatetimeIndex(['2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30', '2022-07-01', '2022-07-02'], dtype='datetime64[ns]', freq='D') 12df = pd.DataFrame(np.random.randn(6,4), index =dates, columns = list('ABCD'))df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2022-06-27 -0.187705 0.396903 -1.553965 -0.920126 2022-06-28 -0.540548 0.040966 0.018852 2.159429 2022-06-29 0.124971 -0.000083 0.941922 0.743876 2022-06-30 0.451639 -0.763349 0.825701 0.030078 2022-07-01 -0.299505 -0.354058 -0.696545 0.062945 2022-07-02 -0.607932 -0.133409 0.707404 1.150122 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-5b526fe9-b6e7-4251-b812-301488a5a15a button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-5b526fe9-b6e7-4251-b812-301488a5a15a'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12345678df2 = pd.DataFrame({'A':1., 'B': pd.Timestamp('20220627'), 'C': pd.Series(1,index = list(range(4)),dtype='float32'), 'D': np.array([3] * 4, dtype = 'int32'), 'E': pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), 'F': 'foo'})print(df2)print(df2.dtypes) A B C D E F 0 1.0 2022-06-27 1.0 3 test foo 1 1.0 2022-06-27 1.0 3 train foo 2 1.0 2022-06-27 1.0 3 test foo 3 1.0 2022-06-27 1.0 3 train foo A float64 B datetime64[ns] C float32 D int32 E category F object dtype: object 1df.tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2022-06-30 0.451639 -0.763349 0.825701 0.030078 2022-07-01 -0.299505 -0.354058 -0.696545 0.062945 2022-07-02 -0.607932 -0.133409 0.707404 1.150122 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-591ec042-efb3-4358-95b3-45eef8a50055 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-591ec042-efb3-4358-95b3-45eef8a50055'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2022-06-28 -0.540548 0.040966 0.018852 2.159429 2022-06-29 0.124971 -0.000083 0.941922 0.743876 2022-06-30 0.451639 -0.763349 0.825701 0.030078 2022-07-01 -0.299505 -0.354058 -0.696545 0.062945 2022-07-02 -0.607932 -0.133409 0.707404 1.150122 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-cc8cacd2-9f16-4f17-a329-22f032213765 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-cc8cacd2-9f16-4f17-a329-22f032213765'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2022-06-27 -0.187705 0.396903 -1.553965 -0.920126 2022-06-28 -0.540548 0.040966 0.018852 2.159429 2022-06-29 0.124971 -0.000083 0.941922 0.743876 2022-06-30 0.451639 -0.763349 0.825701 0.030078 2022-07-01 -0.299505 -0.354058 -0.696545 0.062945 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-c4a00f69-5d4d-4b2b-b265-6908d0c08237 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-c4a00f69-5d4d-4b2b-b265-6908d0c08237'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt;","link":"/2022/06/28/day0628/"},{"title":"","text":"파이썬 Machine Learning 정형데이터 사이킷런 Deep Learning 비정형데이터 Tensorflow(구글) Pytorch(페이스북) 혼공머: Tensorflow 실제 상용서비스 - Tensorflow R&amp;D- Pytorch 생선 분류 도미, 곤들매기, 농어 등등 이 생선들을 프로그램으로 분류한다. -30cm 이상이면 도미라고 알려줘 도미의 길이와 무게를 줬다.12bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] ##데이터 가공 여기서는 생략 ##데이터 시각화 여러인사이트 확인 위해 시각화, 통계 수치 계산 탐색적 자료분석(EDA:Exploratory Data analysis) 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() 파이썬 시각화는 객체지향으로 한다. 1234567import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(bream_length, bream_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 1 빙어 데이터 준비하기 12smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123456fig,ax = plt.subplots()ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 123456fig,ax = plt.subplots()ax.scatter(bream_length,bream_weight)ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 두개의 리스트 합치기 12length = bream_length + smelt_lengthweight = bream_weight + smelt_weight 2차원리스트로 만든다. 12fish_data = [[l,w]for l,w in zip(length,weight)]fish_data[0:5] [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] 라벨링을 해준다. = 지도 해준다.=지도학습 12fish_target = [1]*35 + [0]*14print(fish_target) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 모델링1234567from sklearn.neighbors import KNeighborsClassifier#클래스 인스턴스화kn=KNeighborsClassifier()#모형 학습kn.fit(fish_data,fish_target) 1.0 12# 예측 정확도kn.score(fish_data, fish_target) 실제 에측을 해보자 새로운 물고기 도착했습니다. 길이: 30, 몸무게 : 600 1kn.predict([[30,600]]) array([1]) 123456789ac_length = int(input(&quot;물고기 길이를 입력하세요 ...&quot;))ac_weight = int(input(&quot;물고기 무게를 입력하세요 ...&quot;))preds = kn.predict([[ac_length,ac_weight]])if preds ==1: print(&quot;도미다&quot;)else: print(&quot;빙어다&quot;) 물고기 길이를 입력하세요 ...1 물고기 무게를 입력하세요 ...10 빙어다","link":"/2022/06/29/day29_mi/"},{"title":"","text":"1234import pandas as pdimport numpy as npprint(&quot;pandas version: &quot;,pd.__version__)print(&quot;numpy version: &quot;, np.__version__) pandas version: 1.3.5 numpy version: 1.21.6 데이터 불러오기 구글 드라이브에 데이터 존재 12from google.colab import drivedrive.mount('/content/drive') Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 1234DATA_PATH = ('/content/drive/MyDrive/Colab Notebooks/human_ai/Basic/Chapter 3. pandas/data/Lemonade2016.csv')lemonade=pd.read_csv(DATA_PATH)print(type(lemonade))lemonade.info() &lt;class 'pandas.core.frame.DataFrame'&gt; &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB 데이터 맛보기 1print(lemonade.head()) Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 1print(lemonade.tail(10)) Date Location Lemon Orange Temperature Leaflets Price 22 7/22/2016 Park 112 75 80 108.0 0.50 23 7/23/2016 Park 120 82 81 117.0 0.50 24 7/24/2016 Park 121 82 82 117.0 0.50 25 7/25/2016 Park 156 113 84 135.0 0.50 26 7/26/2016 Park 176 129 83 158.0 0.35 27 7/27/2016 Park 104 68 80 99.0 0.35 28 7/28/2016 Park 96 63 82 90.0 0.35 29 7/29/2016 Park 100 66 81 95.0 0.35 30 7/30/2016 Beach 88 57 82 81.0 0.35 31 7/31/2016 Beach 76 47 82 68.0 0.35 기술 통계랑 보는 함수 describe() 12print(lemonade.describe()) Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354688 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 범주형 데이터 빈도수 구하기value_counts() =&gt; 빈도수를 구하는 함수 1lemonade['Location'].value_counts() Beach 17 Park 15 Name: Location, dtype: int64 행과 열 다루기 sold(판매량) 컬럼(feature)을 추가 12lemonade['Sold'] = 0print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 0 7/1/2016 Park 97 67 70 90.0 0.25 0 1 7/2/2016 Park 98 67 72 90.0 0.25 0 2 7/3/2016 Park 110 77 71 104.0 0.25 0 Revenue 0 0.0 1 0.0 2 0.0 추가한 칼럼에 값을 넣어보기 12lemonade['Sold'] = lemonade['Lemon'] + lemonade['Orange']print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 Revenue 0 0.0 1 0.0 2 0.0 Revenue(매출) = 단가 X 판매량 1234lemonade['Revenue'] = 0lemonade['Revenue'] = lemonade['Sold']*lemonade['Price']print(lemonade.head(3)) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 0 7/1/2016 Park 97 67 70 90.0 0.25 164 1 7/2/2016 Park 98 67 72 90.0 0.25 165 2 7/3/2016 Park 110 77 71 104.0 0.25 187 Revenue 0 41.00 1 41.25 2 46.75 필요한 정보만 뽑아올 때1print(lemonade[['Revenue','Price','Sold']].head()) Revenue Price Sold 0 0.0 0.25 0 1 0.0 0.25 0 2 0.0 0.25 0 3 0.0 0.25 0 4 0.0 0.25 0 drop() 함수 사용해서 열 제거 또는 행 제거 axis =1 =&gt;columns axis =0 =&gt;index 123# 컬럼 제거col_drop = lemonade.drop('Sold', axis=1)print(col_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Revenue 0 7/1/2016 Park 97 67 70 90.0 0.25 41.00 1 7/2/2016 Park 98 67 72 90.0 0.25 41.25 2 7/3/2016 Park 110 77 71 104.0 0.25 46.75 3 7/4/2016 Beach 134 99 76 98.0 0.25 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 69.25 123# 행 제거row_drop = lemonade.drop([0,1,2],axis=0)print(row_drop.head()) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 Revenue 3 58.25 4 69.25 5 43.00 6 43.00 7 61.00 데이터 인덱싱1print(lemonade[4:7]) Date Location Lemon Orange Temperature Leaflets Price Sold \\ 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 Revenue 4 69.25 5 43.00 6 43.00 특정 값만 추출 filter 1lemonade['Location']=='Beach' 0 False 1 False 2 False 3 True 4 True 5 True 6 True 7 True 8 True 9 True 10 True 11 True 12 True 13 True 14 True 15 True 16 True 17 True 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 False 26 False 27 False 28 False 29 False 30 True 31 True Name: Location, dtype: bool 12# 데이터[데이터 컬럼 == 특정값]lemonade[lemonade['Location']=='Beach'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Sold Revenue 3 7/4/2016 Beach 134 99 76 98.0 0.25 233 58.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 277 69.25 5 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 6 7/6/2016 Beach 103 69 82 90.0 0.25 172 43.00 7 7/7/2016 Beach 143 101 81 135.0 0.25 244 61.00 8 NaN Beach 123 86 82 113.0 0.25 209 52.25 9 7/9/2016 Beach 134 95 80 126.0 0.25 229 57.25 10 7/10/2016 Beach 140 98 82 131.0 0.25 238 59.50 11 7/11/2016 Beach 162 120 83 135.0 0.25 282 70.50 12 7/12/2016 Beach 130 95 84 99.0 0.25 225 56.25 13 7/13/2016 Beach 109 75 77 99.0 0.25 184 46.00 14 7/14/2016 Beach 122 85 78 113.0 0.25 207 51.75 15 7/15/2016 Beach 98 62 75 108.0 0.50 160 80.00 16 7/16/2016 Beach 81 50 74 90.0 0.50 131 65.50 17 7/17/2016 Beach 115 76 77 126.0 0.50 191 95.50 30 7/30/2016 Beach 88 57 82 81.0 0.35 145 50.75 31 7/31/2016 Beach 76 47 82 68.0 0.35 123 43.05 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-c518eff9-ccce-49a7-82fe-1f3afb2889e2 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-c518eff9-ccce-49a7-82fe-1f3afb2889e2'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Sold 5 7/6/2016 172 6 7/6/2016 172 7 7/7/2016 244 8 NaN 209 10 7/10/2016 238 11 7/11/2016 282 12 7/12/2016 225 18 7/18/2016 223 23 7/23/2016 202 24 7/24/2016 203 25 7/25/2016 269 26 7/26/2016 305 28 7/28/2016 159 29 7/29/2016 166 30 7/30/2016 145 31 7/31/2016 123 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-2e4f97cb-85b4-4511-aaf5-3d6b3c53b3ae button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-2e4f97cb-85b4-4511-aaf5-3d6b3c53b3ae'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12(lemonade['Temperature'] &gt;= 80) &amp; (lemonade['Orange'] &gt;= 100) 0 False 1 False 2 False 3 False 4 False 5 False 6 False 7 True 8 False 9 False 10 False 11 True 12 False 13 False 14 False 15 False 16 False 17 False 18 False 19 False 20 False 21 False 22 False 23 False 24 False 25 True 26 True 27 False 28 False 29 False 30 False 31 False dtype: bool 1lemonade.loc[lemonade['Temperature']&gt;80,['Date','Sold']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Sold 5 7/6/2016 0 6 7/6/2016 0 7 7/7/2016 0 8 NaN 0 10 7/10/2016 0 11 7/11/2016 0 12 7/12/2016 0 18 7/18/2016 0 23 7/23/2016 0 24 7/24/2016 0 25 7/25/2016 0 26 7/26/2016 0 28 7/28/2016 0 29 7/29/2016 0 30 7/30/2016 0 31 7/31/2016 0 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-c6285936-cd27-4c83-9e4f-981639735d02 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-c6285936-cd27-4c83-9e4f-981639735d02'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 17 loc vs iloc iloc 숫자만 들어감 1print(lemonade.iloc[0:3,0:2]) Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park loc:숫자(X) 라벨= 글자(숫자,문자 동시) 1print(lemonade.loc[0:2,['Date','Location']]) Date Location 0 7/1/2016 Park 1 7/2/2016 Park 2 7/3/2016 Park 데이터 정렬 sort values() 12lemonade.head()print(lemonade[['Date', 'Temperature', 'Revenue']].sort_values(by=['Revenue']).head(5)) Date Temperature Revenue 0 7/1/2016 70 41.00 1 7/2/2016 72 41.25 6 7/6/2016 82 43.00 5 7/6/2016 82 43.00 31 7/31/2016 82 43.05 1lemonade[['Date', 'Temperature', 'Revenue']].sort_values(by=['Temperature','Revenue'],ascending=[True,False]).head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Temperature Revenue 20 7/20/2016 70 56.50 0 7/1/2016 70 41.00 2 7/3/2016 71 46.75 1 7/2/2016 72 41.25 16 7/16/2016 74 65.50 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-e3ba8370-cae8-43bc-9883-17745e7413bf button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-e3ba8370-cae8-43bc-9883-17745e7413bf'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; Group by1 123df= lemonade.groupby('Location').count()print(df)print(type(df)) &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f6449cb2110&gt; &lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt; 1lemonade.groupby('Location')[['Date','Lemon']].count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Lemon Location Beach 16 17 Park 15 15 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-f5cc9ef8-daf5-44ec-8815-19e2fdaf01ae button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-f5cc9ef8-daf5-44ec-8815-19e2fdaf01ae'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1df[['Date','Lemon']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Lemon Location Beach 16 17 Park 15 15 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-316141d6-fe27-4281-ba44-cbb56692bc49 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-316141d6-fe27-4281-ba44-cbb56692bc49'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1print(df.loc['Beach',['Date','Lemon']]) Date 16 Lemon 17 Name: Beach, dtype: int64 1lemonade.groupby('Location')['Revenue'].agg([max,min,sum]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } max min sum Location Beach 95.5 43.0 1002.8 Park 134.5 41.0 1178.2 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-9b42e40a-7b7d-4b8e-9f6a-20c304028ad9 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-9b42e40a-7b7d-4b8e-9f6a-20c304028ad9'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12lemonade.groupby(['Location','Price'])[['Revenue','Sold','Temperature']].agg([max,min,sum,np.mean]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Revenue Sold Temperature max min sum mean max min sum mean max min sum mean Location Price Beach 0.25 70.50 43.00 668.0 55.666667 282 172 2672 222.666667 84 76 965 80.416667 0.35 50.75 43.05 93.8 46.900000 145 123 268 134.000000 82 82 164 82.000000 0.50 95.50 65.50 241.0 80.333333 191 131 482 160.666667 77 74 226 75.333333 Park 0.25 46.75 41.00 129.0 43.000000 187 164 516 172.000000 72 70 213 71.000000 0.35 106.75 55.65 280.7 70.175000 305 159 802 200.500000 83 80 326 81.500000 0.50 134.50 56.50 768.5 96.062500 269 113 1537 192.125000 84 70 633 79.125000 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-cc76c6f6-6aa3-40e5-a778-997898cf5d57 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-cc76c6f6-6aa3-40e5-a778-997898cf5d57'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt;","link":"/2022/06/29/day0629/"},{"title":"","text":"파이썬 Machine Learning 정형데이터 사이킷런 Deep Learning 비정형데이터 Tensorflow(구글) Pytorch(페이스북) 혼공머: Tensorflow 실제 상용서비스 - Tensorflow R&amp;D- Pytorch 생선 분류 도미, 곤들매기, 농어 등등 이 생선들을 프로그램으로 분류한다. -30cm 이상이면 도미라고 알려줘 도미의 길이와 무게를 줬다.12bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0] ##데이터 가공 여기서는 생략 ##데이터 시각화 여러인사이트 확인 위해 시각화, 통계 수치 계산 탐색적 자료분석(EDA:Exploratory Data analysis) 123456import matplotlib.pyplot as pltplt.scatter(bream_length, bream_weight)plt.xlabel('length')plt.ylabel('weight')plt.show() 파이썬 시각화는 객체지향으로 한다. 1234567import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(bream_length, bream_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 1 빙어 데이터 준비하기 12smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 123456fig,ax = plt.subplots()ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 123456fig,ax = plt.subplots()ax.scatter(bream_length,bream_weight)ax.scatter(smelt_length,smelt_weight)ax.set_xlabel('length')ax.set_ylabel('weight')plt.show() 두개의 리스트 합치기 12length = bream_length + smelt_lengthweight = bream_weight + smelt_weight 2차원리스트로 만든다. 12fish_data = [[l,w]for l,w in zip(length,weight)]fish_data[0:5] [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0]] 라벨링을 해준다. = 지도 해준다.=지도학습 12fish_target = [1]*35 + [0]*14print(fish_target) [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 모델링1234567from sklearn.neighbors import KNeighborsClassifier#클래스 인스턴스화kn=KNeighborsClassifier()#모형 학습kn.fit(fish_data,fish_target) KNeighborsClassifier() 12# 예측 정확도kn.score(fish_data, fish_target) 1.0 실제 에측을 해보자 새로운 물고기 도착했습니다. 길이: 30, 몸무게 : 600 1kn.predict([[30,600]]) array([1]) 123456789ac_length = int(input(&quot;물고기 길이를 입력하세요 ...&quot;))ac_weight = int(input(&quot;물고기 무게를 입력하세요 ...&quot;))preds = kn.predict([[ac_length,ac_weight]])if preds ==1: print(&quot;도미다&quot;)else: print(&quot;빙어다&quot;) 물고기 길이를 입력하세요 ...13 물고기 무게를 입력하세요 ...58365 도미다 새로운 모델 제안1234from sklearn.neighbors import KNeighborsClassifierkn49 = KNeighborsClassifier(n_neighbors=49)kn49.fit(fish_data, fish_target)kn49.score(fish_data,fish_target) 0.7142857142857143 하이퍼 파라미터 세팅 n_neighbors = 49 default : 100% 머신러닝 알고리즘 두 개의 흐름 선형 모델: 선형 회귀, 로지스틱 회귀, 서포트 벡터 머신 의사결정트리 모델: 1975년 의사결정트리 모델, KNN, 랜덤포레스트 부스팅계열 : LightGBM(2017), XGBoost(2016) 선형회귀, 로지스틱회귀, 랜덤포레스트, LightGBM(=XGBoost) 12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 2차원 파이썬 리스트 라벨링 123456fish_data = [[l,w]for l,w in zip(fish_length,fish_weight)]fish_target = [1]*35 + [0]*14print(fish_target[0:40:5])print(fish_data[0:40:5]) [1, 1, 1, 1, 1, 1, 1, 0] [[25.4, 242.0], [29.7, 450.0], [31.0, 475.0], [32.0, 600.0], [34.0, 575.0], [35.0, 725.0], [38.5, 920.0], [9.8, 6.7]] Sample 도미 35마리, 빙어 14마리 49개의 샘플 존재 처음 35개를 훈련 /나머지 14개를 테스트 12345678910111213from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()#훈련 세트로 0:34 인덱스 활용train_input = fish_data[:35]train_target = fish_target[:35]#테스트 세트로 35:마지막 인덱스 활용test_input = fish_data[35:]test_target = fish_target[35:]#모형 학습kn=kn.fit(train_input,train_target)print(kn.score(test_input,test_target)) 0.0 샘플링 편향 훈련 세트와 테스트 세트가 골고루 섞이지 않음 샘플링 작업123456import numpy as npinput_arr = np.array(fish_data)target_arr = np.array(fish_target)print(input_arr[0:49:7])print(input_arr.shape, target_arr.shape) [[ 25.4 242. ] [ 30. 390. ] [ 32. 600. ] [ 34. 685. ] [ 36. 850. ] [ 9.8 6.7] [ 11.8 9.9]] (49, 2) (49,) 1target_arr array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) 123456# random으로 무작위 배열을 만들 때index = np.arange(49) #0~48 생성np.random.shuffle(index) # 섞어np.random.seed(42) #고정print(index) [13 45 47 44 17 27 26 25 31 19 12 4 34 8 3 6 40 41 46 15 9 16 24 33 30 0 43 32 5 29 11 36 1 21 2 37 35 23 39 10 22 18 48 20 7 42 14 28 38] train_input = input_arr[index[:35]]train_target = target_arr[index[:35]] test_input = input_arr[index[35:]]test_target = target_arr[index[35:]] 12345train_input = input_arr[index[:35]]train_target = target_arr[index[:35]]test_input = input_arr[index[35:]]test_target = target_arr[index[35:]] 123print(train_input)print()print(test_input) [[ 29.7 500. ] [ 12.2 12.2] [ 33. 700. ] [ 11.3 8.7] [ 39.5 925. ] [ 29. 430. ] [ 36. 714. ] [ 36. 850. ] [ 31. 475. ] [ 35. 720. ] [ 37. 1000. ] [ 11.2 9.8] [ 34.5 620. ] [ 12. 9.8] [ 29. 363. ] [ 33. 700. ] [ 30.7 500. ] [ 38.5 955. ] [ 33.5 650. ] [ 14.3 19.7] [ 31.5 500. ] [ 25.4 242. ] [ 9.8 6.7] [ 32. 600. ] [ 10.5 7.5] [ 33.5 610. ] [ 10.6 7. ] [ 35. 700. ] [ 32. 600. ] [ 35. 725. ] [ 13. 12.2] [ 30. 450. ] [ 32. 340. ] [ 15. 19.9] [ 30. 390. ] [ 41. 975. ]] [[ 11. 9.7] [ 11.8 9.9] [ 29.7 450. ] [ 11.8 10. ] [ 26.5 340. ] [ 12.4 13.4] [ 34. 685. ] [ 34. 575. ] [ 38.5 920. ] [ 35. 680. ] [ 26.3 290. ] [ 31. 500. ] [ 41. 950. ]] 1train_input[:,0] array([29.7, 12.2, 33. , 11.3, 39.5, 29. , 36. , 36. , 31. , 35. , 37. , 11.2, 34.5, 12. , 29. , 33. , 30.7, 38.5, 33.5, 14.3, 31.5, 25.4, 9.8, 32. , 10.5, 33.5, 10.6, 35. , 32. , 35. , 13. , 30. , 32. , 15. , 30. , 41. ]) 123456789import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(train_input[:,0], train_input[:,1])ax.scatter(test_input[:,0], test_input[:,1])ax.set_xlabel('length')ax.set_ylabel('weight')plt.show()#파란색이 train 데이터#주황색이 test 데이터 두번째 머신러닝 프로그램12kn.fit(train_input, train_target) # 훈련 시키고kn.score(test_input,test_target) # 실제 데이터를 넣어서 어느정도 정확도를 보이는지 확인한다. 1.0 1kn.predict(test_input) # train으로 훈련을 하고 그 결과를 바탕으로 test_input을 넣어서 예측을 해본 값 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) 1test_target # 실제 데이터 array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]) ##데이터의 전처리 머신러닝 시, 데이터 전처리 결측치 처리, 이상치 처리 12345678fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] 12# column_stack() 활용np.column_stack(([1,2,3],[4,5,6])) array([[1, 4], [2, 5], [3, 6]]) 독립 변수 12fish_data = np.column_stack((fish_length,fish_weight)) 종속변수 = Y = 타깃 데이터 = Target 12fish_target = np.concatenate((np.ones(35),np.zeros(14)))print(fish_target) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 1np.ones(35) array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) 1np.zeros(14) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) scikit-learn 훈련세트와 테스트 세트 나누기123456from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_data,fish_target,random_state =42 #fish_data와 fish_target으로 random 분류한다.)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) 12345678910111213from sklearn.neighbors import KNeighborsClassifier# 클래스 인스턴스화kn = KNeighborsClassifier()#훈련 세트로 0:34 인덱스 활용train_input = fish_data[:35]train_target = fish_target[:35]#테스트 세트로 35:마지막 인덱스 활용test_input = fish_data[35:]test_target = fish_target[35:]#모형 학습kn=kn.fit(train_input,train_target)print(kn.score(test_input,test_target)) p92 도미와 빙어가 잘 섞여 있나? 1print(test_target) [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] 25(도미) : 14(빙어) 2.5:1 테스트 셋(비율) 3.3 : 1 123456from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_data,fish_target,stratify=fish_target,random_state =42)train_input.shape, test_input.shape, train_target.shape, test_target.shape ((36, 2), (13, 2), (36,), (13,)) 층화샘플링 기초 통계, 설문조사 비율 여론 조사 수상한 도미 한 마리- 1234from sklearn.neighbors import KNeighborsClassifierkn=KNeighborsClassifier()kn.fit(train_input,train_target)kn.score(test_input, test_target) 1.0 도미 사이즈 20cm 이상 = 1 빙어 사이즈 1print(kn.predict([[25,150]])) [0.] 1train_input[:,0] array([29.7, 12.2, 33. , 11.3, 39.5, 29. , 36. , 36. , 31. , 35. , 37. , 11.2, 34.5, 12. , 29. , 33. , 30.7, 38.5, 33.5, 14.3, 31.5, 25.4, 9.8, 32. , 10.5, 33.5, 10.6, 35. , 32. , 35. , 13. , 30. , 32. , 15. , 30. , 41. ]) 12345678import matplotlib.pyplot as pltfig, ax =plt.subplots()ax.scatter(train_input[:,0],train_input[:,1])ax.scatter(25,150,marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 12345678import matplotlib.pyplot as pltplt.scatter(train_input[:,0], train_input[:,1])plt.scatter(25, 150, marker='^')plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')plt.xlim((0, 1000))plt.xlabel('length')plt.ylabel('weight')plt.show() p98 두 특성(길이와 무게)의 값이 놓인 범위가 매우 다름 두 특성의 스케일이 다름 스케일이 같도록 통계 처리 필요 Feature Engineering 머신러닝 데이터 전처리 (결측치 처리, 이상치 처리) 데이터 분리 Feature Engineering 표준 점수 점수 12train_input# 여기서 세로로 평균을 내는 것이 axis= 0 array([[ 29.7, 500. ], [ 12.2, 12.2], [ 33. , 700. ], [ 11.3, 8.7], [ 39.5, 925. ], [ 29. , 430. ], [ 36. , 714. ], [ 36. , 850. ], [ 31. , 475. ], [ 35. , 720. ], [ 37. , 1000. ], [ 11.2, 9.8], [ 34.5, 620. ], [ 12. , 9.8], [ 29. , 363. ], [ 33. , 700. ], [ 30.7, 500. ], [ 38.5, 955. ], [ 33.5, 650. ], [ 14.3, 19.7], [ 31.5, 500. ], [ 25.4, 242. ], [ 9.8, 6.7], [ 32. , 600. ], [ 10.5, 7.5], [ 33.5, 610. ], [ 10.6, 7. ], [ 35. , 700. ], [ 32. , 600. ], [ 35. , 725. ], [ 13. , 12.2], [ 30. , 450. ], [ 32. , 340. ], [ 15. , 19.9], [ 30. , 390. ], [ 41. , 975. ]]) 1234mean = np.mean(train_input, axis =0)std = np.std(train_input, axis =0)print(mean, std) [ 27.29722222 454.09722222] [ 9.98244253 323.29893931] 표준 점수 구하기 123# 브로드캐스팅 서로 다른 배열을 계산할 때print(train_input.shape, mean.shape, std.shape)train_scaled = (train_input-mean)/std (36, 2) (2,) (2,) 12345plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(25, 150, marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 123456new = ([25, 150] - mean) / stdplt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.xlabel('length')plt.ylabel('weight')plt.show() 통계처리 전 : KNN -&gt; 예측이 틀림통계처리 후: KNN –&gt; 예측이 정확하게 맞음–&gt; 통계처리 –&gt; Feature Engineering 1kn.fit(train_scaled, train_target) KNeighborsClassifier() 12test_scaled = (test_input -mean)/stdkn.score(test_scaled, test_target) 1.0 예측 1print(kn.predict([new])) [1.] 1234567distances, indexes = kn.kneighbors([new])plt.scatter(train_scaled[:,0], train_scaled[:,1])plt.scatter(new[0], new[1], marker='^')plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')plt.xlabel('length')plt.ylabel('weight')plt.show()","link":"/2022/06/30/day30_mi/"},{"title":"","text":"로지스틱 회귀 선형 회귀에서 출발 이진 분류 문제 해결 클래스 확률 예측 딥러닝에서도 사용됨 ##교재 177 X가 사각형일 확률 30% X가 삼각형일 확률 50% X가 원일 확률 20% 데이터 불러오기 Species (종속변수 = Y) 독립변수 Weight, Length, Diagonal, Height, Width 1234import pandas as pdfish = pd.read_csv('https://bit.ly/fish_csv_data') # 데이터를 불러와라fish.head() # 일단 5개만 출력을 하는데 이름 순서대로 정리가 되어 있다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Species Weight Length Diagonal Height Width 0 Bream 242.0 25.4 30.0 11.5200 4.0200 1 Bream 290.0 26.3 31.2 12.4800 4.3056 2 Bream 340.0 26.5 31.1 12.3778 4.6961 3 Bream 363.0 29.0 33.5 12.7300 4.4555 4 Bream 430.0 29.0 34.0 12.4440 5.1340 5 Bream 450.0 29.7 34.7 13.6024 4.9274 6 Bream 500.0 29.7 34.5 14.1795 5.2785 7 Bream 390.0 30.0 35.0 12.6700 4.6900 8 Bream 450.0 30.0 35.1 14.0049 4.8438 9 Bream 500.0 30.7 36.2 14.2266 4.9594 10 Bream 475.0 31.0 36.2 14.2628 5.1042 11 Bream 500.0 31.0 36.2 14.3714 4.8146 12 Bream 500.0 31.5 36.4 13.7592 4.3680 13 Bream 340.0 32.0 37.3 13.9129 5.0728 14 Bream 600.0 32.0 37.2 14.9544 5.1708 15 Bream 600.0 32.0 37.2 15.4380 5.5800 16 Bream 700.0 33.0 38.3 14.8604 5.2854 17 Bream 700.0 33.0 38.5 14.9380 5.1975 18 Bream 610.0 33.5 38.6 15.6330 5.1338 19 Bream 650.0 33.5 38.7 14.4738 5.7276 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1 데이터 탐색123# 종속변수print(pd.unique(fish['Species'])) # 물고기의 종류는?print(fish['Species'].value_counts()) # 각 물고기마다 몇 마리씩 있는가? ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt'] Perch 56 Bream 35 Roach 20 Pike 17 Smelt 14 Parkki 11 Whitefish 6 Name: Species, dtype: int64 123#pandas 데이터 프레임에서 numpy 배열로 변환fish_input = fish[['Weight', 'Length','Diagonal','Height','Width']].to_numpy()fish_input.shape (159, 5) 1print(fish_input[:5]) [[242. 25.4 30. 11.52 4.02 ] [290. 26.3 31.2 12.48 4.3056] [340. 26.5 31.1 12.3778 4.6961] [363. 29. 33.5 12.73 4.4555] [430. 29. 34. 12.444 5.134 ]] 123fish_target=fish['Species'].to_numpy() #'Species'만 배열로 만들어라print(fish_target.shape) #Species 만 배열로 만들었을 때 몇 바이 몇이 되는가?print(fish_target[:5])# 상위 5개를 추출해봐라 (159,) ['Bream' 'Bream' 'Bream' 'Bream' 'Bream'] 데이터 분리 훈련 데이터 테스트 데이터 분리 123456from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state =42) #fish_input, fish_target 을 넣어서 랜덤으로 train_input~ test_target 을 생성한다.# 층화샘플링 #표준화 전처리 여기에서도 훈련 세트의 통계 값으로 테스트 세트를 변환해야한다는 점을 기억해라 데이터 가공 숫자 결측치가 존재, 평균값으로 대체 원본 데이터 평균 대치(x) 훈련 데이터와 테스트 데이터 분리 12import numpy as npnp.mean(train_input[:, 2]) 31.269747899159658 1234567from sklearn.preprocessing import StandardScaler ss = StandardScaler()ss.fit(train_input)# ss.fit(test_input) (X)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) ##모형 만들기 K 최근접 이웃 12345from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors =3) # 5개가 아니라 근접 3개만kn.fit(train_scaled, train_target) #훈련 시켜라print(kn.score(train_scaled, train_target)) # 훈련 시키니 정확도가 어느정도니?print(kn.score(test_scaled, test_target)) # 훈련 시킨 것을 바탕으로 실제로 test를 돌려보니 정확도가 어느정도니? 0.8907563025210085 0.85 타깃값 확인 1print(kn.classes_) # 물고기 종류에는 어떤 것들이 있니? ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 1test_scaled[:5] # 각 열은 특징들에 대한 점수라고 생각 총 행은 5개 array([[-0.88741352, -0.91804565, -1.03098914, -0.90464451, -0.80762518], [-1.06924656, -1.50842035, -1.54345461, -1.58849582, -1.93803151], [-0.54401367, 0.35641402, 0.30663259, -0.8135697 , -0.65388895], [-0.34698097, -0.23396068, -0.22320459, -0.11905019, -0.12233464], [-0.68475132, -0.51509149, -0.58801052, -0.8998784 , -0.50124996]]) 1print(kn.predict(test_scaled[:5])) # 5개에 대한 예측값을 말해봐라 ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] 5개 샘플에 대한 예측은 어떤 확률이냐? 123import numpy as npproba = kn.predict_proba(test_scaled[:5])print(np.round(proba,decimals=4)) [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] 첫번째 클래스 Perch [0. 0. 1. 0. 0. 0. 0. } 네번 째 클래스 perch 66.7 확률로 perch로 예측 1 ##회귀식-양변에 로그를 취함 y=ax 로지스틱 회귀로 이진 분류 수행12char_arr = np.array(['A','B','C','D','E']) # a,b,c,d,e에서print(char_arr[[True, False, True, False, False]]) # 각 값에 참 거짓을 넣어주고, 참만 출력한다. ['A' 'C'] 12345bream_smelt_indexes = (train_target == 'Bream')|(train_target == 'Smelt') # 참 값만 고른다train_bream_smelt = train_scaled[bream_smelt_indexes] # train_bream_smelt 에 bream이거나 smelt 인 값만 넣고~target_bream_smelt = train_target[bream_smelt_indexes]# target 또한 같은 일을 반복한다.train_scaled.shape, train_bream_smelt.shape # 모양을 확인하자 , train은 119개 test 는 33개가 있다. ((119, 5), (33, 5)) 모델 만들기 1234from sklearn.linear_model import LogisticRegression lr= LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt)# 훈련시켜라 LogisticRegression() 12# 클래스를 예측print(lr.predict(train_bream_smelt[:5])) # 상위 5개의 예측값을 출력해봐라 ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] -확률 값 구하기 1print(lr.predict_proba(train_bream_smelt[:5])) # 그럼 각 확률을 출력해봐라 breamd의 확률, smelt 의 확률 [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] 분류 기준 : threshold 임계값 설정도미 vs 빙어[0.51,0.49],[0.90,0.10] 1print(lr.classes_) ['Bream' 'Smelt'] 계수와 절편 1print(lr.coef_, lr.intercept_) # 앞의 5개는 weight의 계수, length의 계수 등등이고 마지막은 교차점이다. [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] 12decisions= lr.decision_function(train_bream_smelt[:5])print(decisions) [-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] 12from scipy.special import expit # 시그모이드 함수를 적용한 이후의 모습이다. z5개를 돌렸기에 0~1사이의 값 5개가 나온 것이다.print(expit(decisions)) [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 다중 분류 수행하기123456#하이퍼 파라밑 세팅 =&gt; 모형을 튜닝#모형 결과의 과대적합 또는 과소적합을 방지 하기 위한 것lr = LogisticRegression(C=20,max_iter =1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target)) 0.9327731092436975 0.925 5개 샘플에 대한 예측 1print(lr.predict(test_scaled[:5])) # 5개 샘플에대한 예측값 ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 12proba= lr.predict_proba(test_scaled[:5])print(np.round(proba,decimals = 3)) [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 다중 분류일 경우 선형 방정식은 어떤 모습일까? 분류 7개, 컬럼 값 5개 123print(lr.coef_,lr.intercept_) #각각의 계수와 교차점# z = 이 형식을 의미하는 것이다. [[-1.49002087 -1.02912886 2.59345551 7.70357682 -1.2007011 ] [ 0.19618235 -2.01068181 -3.77976834 6.50491489 -1.99482722] [ 3.56279745 6.34357182 -8.48971143 -5.75757348 3.79307308] [-0.10458098 3.60319431 3.93067812 -3.61736674 -1.75069691] [-1.40061442 -6.07503434 5.25969314 -0.87220069 1.86043659] [-1.38526214 1.49214574 1.39226167 -5.67734118 -4.40097523] [ 0.62149861 -2.32406685 -0.90660867 1.71599038 3.6936908 ]] [-0.09205179 -0.26290885 3.25101327 -0.14742956 2.65498283 -6.78782948 1.38422358] 1234decision = lr.decision_function(test_scaled[:5])print(np.round(decision,decimals=2))# 하나의 검사값에 대해 7개의 z값이 나오고, 총 5개(검사한 갯수)가 나온다 [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] 2개 일 때는 시그모이드 함수를 사용해서 z 값을 0~1 사이의 값으로 변환하였지만 지금은 z값이 7개나 되므로 시그모이드가 아닌 소프트맥스 함수를 사용한다. 123from scipy.special import softmax # 시그모이드가 아닌 softmax를 사용한다.proba = softmax(decision, axis=1)print(np.round(proba, decimals=3)) [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 평가지표 회귀 평가지표 결정계수 1-(타깃-예측)^2의 합 / (타깃-평균)^2합 MAE, MSE, RMSE (실제-예측) = 오차 MAE(Mean Absolute Error): 오차의 절댓값의 평균 MSE(Mean Squared Error): 오차 제곱의 평균 RMSE(Root Mean Squared Error): MSE에 제곱근을 취한 값 좋은 모델이란? 결정 계수: 1에 수렴하면 좋은 모델 MAE, MSE, RMSE : 0에 수렴하면 좋은 모델 12345678910111213141516import numpy as npfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_scoretrue = np.array([1, 2, 3, 2, 3, 5, 4, 6, 5, 6, 7, 8, 8]) # 실제값preds = np.array([1, 1, 2, 2, 3, 4, 4, 5, 5, 7, 7, 6, 8]) # 예측값#절대값 오차의 평균mae= mean_absolute_error(true,preds)print(mae)# 제곱 오차의 평균mse = mean_squared_error(true,preds)print(mse)# mse 제곱근rmse = np.sqrt(mse)print(rmse)# 결정계수r2= r2_score(true,preds)print(r2) 0.5384615384615384 0.6923076923076923 0.8320502943378437 0.8617021276595744 분류 평가지표 오차행렬 실제 값+[빙어, 도미, 도미, 빙어, 도미]+[빙어, 빙어, 도미, 빙어, 빙어] TP(빙어를 빙어로 예측): 2 TN(도미를 도미로 예측): 1 FN(실제 도미, 예측 빙어): 2 FP(실제 빙어, 예측 도미): 0 모형의 정확도 3/5 = 60 % TP, TN, FP, FN 정확도: 전체에서 맞춘 갯수 정밀도: 양성이라고 예측한 값 중 실제 양성인 값 재현율: 실제 양성 값 중 양성으로 예측한 값의 비율 로그손실 ROC Curve(=AUC)","link":"/2022/07/01/day0701/"},{"title":"","text":"확률적 경사 하강법 점진적 학습(step, 보폭 학습률 XGBoost, LightGBM, 딥러닝(이미지 분류, 자연어 처리,옵티마이저) ###신경망 이미지 데이터, 자연어 자율주행 자율주행 하루 데이터 1TB -&gt;학습 한꺼번에 다 모델을 학습 어려움 샘플링, 배치, 에포크, 오차(=손실 = loss)가 가장 작은 지점을 찾아야 한다. 1 확률적으로 ,확률적 검사 하강법 손실함수 로지스틱 손실 함 123import pandas as pdfish = pd.read_csv(&quot;https://bit.ly/fish_csv_data&quot;)fish.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 159 entries, 0 to 158 Data columns (total 6 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Species 159 non-null object 1 Weight 159 non-null float64 2 Length 159 non-null float64 3 Diagonal 159 non-null float64 4 Height 159 non-null float64 5 Width 159 non-null float64 dtypes: float64(5), object(1) memory usage: 7.6+ KB 1 입력 데이터와 타깃 데이터 분리 123fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy()fish_input.shape, fish_target.shape ((159, 5), (159,)) 1234from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split( fish_input, fish_target, random_state=42) 훈련 세트와 테스트 세트의 특성 표준화 무게, 길이, 대각선 길이, 높이, 너비 표준화 처리 진 12345678from sklearn.preprocessing import StandardScalerss= StandardScaler()ss.fit(train_input)train_scaled=ss.transform(train_input)test_scaled = ss.transform(test_input)# train_scaled[:5] 모델링 확률적 경사 하강법 123456from sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss = 'log', max_iter = 10, random_state=42)sc.fit(train_scaled, train_target)print(sc.score(train_scaled, train_target))print(sc.score(test_scaled,test_target)) 0.773109243697479 0.775 /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, partial_fit 메서드 사용하면 추가 학 123sc.partial_fit(train_scaled, train_target)print(sc.score(train_scaled,train_target))print(sc.score(test_scaled,test_target)) 0.8151260504201681 0.85 에포크와 과대/과소 적합 에포크 숫자가 적으면 –&gt; 덜 학습 early_stopping 에포크 숫자를 1000, 손실 10, 9 ,8,….,3 3에 도달한 시점이 150 12345import numpy as npsc = SGDClassifier(loss='log', random_state =42)train_score = []test_score=[]classes = np.unique(train_target) 300번 에포크 훈련을 반복훈련할 때마다, train_score, test_score를 추가한다.1234for _ in range(0,300): sc.partial_fit(train_scaled,train_target,classes=classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled,test_target)) 123456789# 시각화import matplotlib.pyplot as pltplt.plot(train_score)plt.plot(test_score)plt.xlabel('epoch')plt.ylabel('accuracy')plt.show()# 앞부분에 테스트 성능이 더 좋은것은 과소적합# 100 이후부터는 과대적합? XGBoost, LightGBM 코드 train -loss, train-accuracy, test-loss, test-accuracy 1 123import pandas as pdwine=pd.read_csv('https://bit.ly/wine_csv_data') 1wine.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol sugar pH class 0 9.4 1.9 3.51 0.0 1 9.8 2.6 3.20 0.0 2 9.8 2.3 3.26 0.0 3 9.8 1.9 3.16 0.0 4 9.4 1.9 3.51 0.0 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1wine.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6497 entries, 0 to 6496 Data columns (total 4 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 alcohol 6497 non-null float64 1 sugar 6497 non-null float64 2 pH 6497 non-null float64 3 class 6497 non-null float64 dtypes: float64(4) memory usage: 203.2 KB 1wine.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol sugar pH class count 6497.000000 6497.000000 6497.000000 6497.000000 mean 10.491801 5.443235 3.218501 0.753886 std 1.192712 4.757804 0.160787 0.430779 min 8.000000 0.600000 2.720000 0.000000 25% 9.500000 1.800000 3.110000 1.000000 50% 10.300000 3.000000 3.210000 1.000000 75% 11.300000 8.100000 3.320000 1.000000 max 14.900000 65.800000 4.010000 1.000000 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-bfadddc7-f809-497f-8df3-d3dd992f496e button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-bfadddc7-f809-497f-8df3-d3dd992f496e'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1234data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy() 12from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split(data,target,test_size=0.2,random_state=42) 1print(train_input.shape,test_input.shape) (5197, 3) (1300, 3) StandardScalar 클래스를 사용해 훈련 세트 전처 12345from sklearn.preprocessing import StandardScalerss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) 12345from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_scaled, train_target)print(lr.score(train_scaled,train_target))print(lr.score(test_scaled, test_target)) 0.7808350971714451 0.7776923076923077 1print(lr.coef_,lr.intercept_) [[ 0.51270274 1.6733911 -0.68767781]] [1.81777902] 12345678910from sklearn.tree import DecisionTreeClassifierimport matplotlib.pyplot as pltfrom sklearn.tree import plot_treedt = DecisionTreeClassifier(max_depth=7,random_state =42)dt.fit(train_scaled,train_target)print(dt.score(train_scaled, train_target))print(dt.score(test_scaled,test_target))plt.figure(figsize=(10,7))plot_tree(dt,max_depth=10,filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 0.8895516644217818 0.8630769230769231 123plt.figure(figsize=(10,7))plot_tree(dt, max_depth =1, filled=True, feature_names=['alcohol','sugar','pH'])plt.show() 불순도 비율 레드와인 5:5 화이트와인 한 범주 안에서 서로 다른 데이터가 얼마나 섞여 있는지 나타냄 흰새과 검은색이 각각 4-개씩 있다. 불순도 최대 0.5 흰색과 검은색이 완전 100% 분리가 됨 흰색 노드 불순도 최소 0 검은색 노드 불순도 최소0 엔드로피 불확실한 정도를 의미함, 0~1 사이 흰색과 검은색이 각각 50개씩 섞여 있다. 엔트로피 최대 1 흰색과 검은색이 완전 100% 분리됨 1print(dt.feature_importances_) [0.16949576 0.67274329 0.15776095] 현업에서 DEcisionTreeClassifier 랜덤 포레스트, XGBoost 하이퍼파라미터 매우 많음 검증 세트 훈련세트와 테스트세트 훈련: 교과서 공부하는 것 훈련세트, 모의평가 검증: 강남대성 모의고사 문제지 테스트: 6월 9월 실전: 수능 123import pandas as pdwine=pd.read_csv('https://bit.ly/wine_csv_data') 1234data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy()from sklearn.model_selection import train_test_splittrain_input, test_input, train_target, test_target = train_test_split(data,target,test_size=0.2,random_state=42) 1sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2,random_state=42) 1print(sub_input.shape, val_input.shape) (4157, 3) (1040, 3) 123456from sklearn.tree import DecisionTreeClassifierdt = DecisionTreeClassifier(random_state =42)dt.fit(sub_input, sub_target)print(&quot;훈련성과:&quot;,dt.score(sub_input, sub_target))print(&quot;검증성과:&quot;,dt.score(val_input, val_target))print(&quot;마지막 최종:&quot;,dt.score(test_input, test_target)) 훈련성과: 0.9971133028626413 검증성과: 0.864423076923077 마지막 최종: 0.8569230769230769 교차 검증 데이터 셋을 반복 분할 For loop 샘플링 편향적일 수 있음 교차검증을 한다고 해서, 정확도가 무조건 올라가는 것은 아니다. 모형을 안정적으로 만들어준다. 과대적합 방지 12345678910import numpy as npfrom sklearn.model_selection import KFolddf = np.array([1,2,3,4,5,6,7,8,9,10])#데이터를 K폴드로 나눈다.folds = KFold(n_splits = 5,shuffle =True)for train_idx, valid_idx in folds.split(df): print(f'훈련 데이터: {df[train_idx]},검증데이터 : {df[valid_idx]}') 훈련 데이터: [ 1 2 3 4 5 6 8 10],검증데이터 : [7 9] 훈련 데이터: [ 1 2 3 4 6 7 9 10],검증데이터 : [5 8] 훈련 데이터: [ 3 4 5 6 7 8 9 10],검증데이터 : [1 2] 훈련 데이터: [1 2 3 5 6 7 8 9],검증데이터 : [ 4 10] 훈련 데이터: [ 1 2 4 5 7 8 9 10],검증데이터 : [3 6] 1234from sklearn.model_selection import cross_validatescores = cross_validate(dt, train_input, train_target)print(scores)print(&quot;평균: &quot;, np.mean(scores['test_score'])) {'fit_time': array([0.01039791, 0.01018906, 0.01092386, 0.01160955, 0.01004958]), 'score_time': array([0.0053134 , 0.00104594, 0.00103641, 0.00105906, 0.00095129]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균: 0.855300214703487 StratifiedKFold 사용 1234from sklearn.model_selection import StratifiedKFoldscores = cross_validate(dt, train_input, train_target,cv=StratifiedKFold())print(scores)print(&quot;평균 : &quot;,np.mean(scores['test_score'])) {'fit_time': array([0.00961757, 0.0068903 , 0.00719047, 0.00706291, 0.00724101]), 'score_time': array([0.00066686, 0.00058341, 0.0006268 , 0.00061035, 0.00061965]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])} 평균 : 0.855300214703487 123splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state =42)scores = cross_validate(dt, train_input, train_target, cv= splitter)print(np.mean(scores['test_score'])) 0.8574181117533719 하이퍼파라미터 튜닝(모델이 학습할 수 없어서 사용자가 지정해야만 하는 파라미터) 그리드 서치 사람이 수동적으로 입력 max_depth:[1,3,5] 랜덤 서치 사람이 범위만 지정 max_depth: 1부터 10 사이 아무거나 베이지안 옵티마이제이션 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술을 AutoML이라고 함 예: PyCaret 각 모델마다 적게는 12개에서, 많게는 56개의 매개변수를 제공한다. XGBoost 100개 하이퍼파라미터와 동시에 교차검증을 수행 교차검증 5번 교차검증 1번 동 때, Max Depth 3번 적용 총 결과값 3 X 5 X 2 나옴 Max Depth = 1,3,7 Criterion = gini,entropy 12345678from sklearn.model_selection import GridSearchCVparams={ 'min_impurity_decrease': [0.0001, 0.0002,0.0003,0.0004,0.0005]}gs = GridSearchCV(DecisionTreeClassifier(random_state =42), params, n_jobs=-1)gs.fit(train_input, train_target) GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}) 123print(&quot;best : &quot;, gs.best_estimator_)dt = gs.best_estimator_print(dt.score(train_input, train_target)) best : DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42) 0.9615162593804117 1print(gs.best_params_) {'min_impurity_decrease': 0.0001} 1print(gs.cv_results_['mean_test_score']) [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605] 12best_index = np.argmax(gs.cv_results_['mean_test_score'])print(gs.cv_results_['params'][best_index]) {'min_impurity_decrease': 0.0001} 1234params = {'min_impurity_decrease': np.arange(0.0001,0.001,0.0001), 'max_depth': range(5,20,1), 'min_samples_split':range(2,100,10) } 12gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = -1)gs.fit(train_input, train_target) GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1, param_grid={'max_depth': range(5, 20), 'min_impurity_decrease': array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009]), 'min_samples_split': range(2, 100, 10)}) 1print(gs.best_params_) {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12} 1print(np.max(gs.cv_results_['mean_test_score'])) 0.8683865773302731 1from scipy.stats import uniform, randint 12rgen = randint(0,10)rgen.rvs(10) array([1, 6, 4, 3, 9, 6, 2, 8, 3, 0]) 1np.unique(rgen.rvs(1000), return_counts = True) (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 92, 107, 99, 95, 103, 120, 104, 86, 85, 109])) 12ugen = uniform(0,1)ugen.rvs(10) array([0.74572568, 0.67723448, 0.2947353 , 0.65314677, 0.71850518, 0.45742993, 0.13068961, 0.15581053, 0.29705039, 0.32323251]) 1234params = {'min_impurity_decrease': uniform(0.0001,0.001), 'max_depth': randint(20,50), 'min_samples_split': randint(2,25), 'min_samples_leaf': randint(1,25),} 1234from sklearn.model_selection import RandomizedSearchCVgs = RandomizedSearchCV(DecisionTreeClassifier(random_state =42), params, n_iter=100, n_jobs =-1, random_state=42)gs.fit(train_input, train_target) RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_iter=100, n_jobs=-1, param_distributions={'max_depth': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b8250&gt;, 'min_impurity_decrease': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe66a065110&gt;, 'min_samples_leaf': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b8bd0&gt;, 'min_samples_split': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b83d0&gt;}, random_state=42) 1print(gs.best_params_) {'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13} 1print(np.max(gs.cv_results_['mean_test_score'])) 0.8695428296438884 12dt = gs.best_estimator_print(dt.score(test_input, test_target)) 0.86","link":"/2022/07/05/day0704/"},{"title":"","text":"1 랜덤 포레스트 Decision TRee(나무 1개) 여러 개 심음 샘플링 Feature Importance 예측해야 할 행의 갯수, 100만개 컬럼의 갯수 200개 -&gt; 100개 나무 100개를 심고 평균을 내자 나무 1개 당 컬럼을 10개로 T1 ame : 20 / T2 mae : 30 / T3 mae 10,… T1 ~ T100 ame : 200개 featrue importances 샘플링 : 부트스트랩 샘플 (복원 추출) 1 123456789101112131415161718192021222324252627import numpy as npimport pandas as pdfrom sklearn.model_selection import train_test_split, cross_validatefrom sklearn.ensemble import RandomForestClassifierwine = pd.read_csv('https://bit.ly/wine_csv_data')data = wine[['alcohol','sugar','pH']].to_numpy()target = wine['class'].to_numpy()# 훈련 데이터train_input, test_input, train_target, test_target = train_test_split( data, target, test_size = 0.2, random_state=42)# 모델링rf = RandomForestClassifier(n_jobs =-1, random_state=42)#모형 평가scores = cross_validate(rf, train_input, train_target, return_train_score= True, n_jobs=-1)print(np.mean(scores['train_score']),np.mean(scores['test_score']))#특성 중요rf.fit(train_input, train_target)print(rf.feature_importances_)#OOBrf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)rf. fit(train_input, train_target)print(rf.oob_score_) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 0.9973541965122431 0.8905151032797809 [0.23167441 0.50039841 0.26792718] 0.8934000384837406 그레이디언트 부스팅 경사하강법의 원리를 이용함 T1 ~TN 증가하면서 오차를 보정해주며 정확성을 높임 랜덤포레스트와의 차이점 랜덤포레스트는 각 나무간 상호 연관성 x 부스팅은 각 나무간 상호 연관성 O 단점 속도가 너무 느림 대안 XGBoost, LightGBM 123456from sklearn.ensemble import GradientBoostingClassifiergb = GradientBoostingClassifier(n_estimators = 500,learning_rate =0.2,random_state =42)scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs=-1)print(np.mean(scores['train_score']), np.mean(scores['test_score'])) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 0.9464595437171814 0.8780082549788999 특성 중요도 12gb.fit(train_input, train_target)print(gb.feature_importances_) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); [0.15872278 0.68010884 0.16116839] 1 주성분 분석 이론적으로는 어려움 좌표계 공간 개념 직교 + 회전 공분산 등(통계 관련 내용) Feature Engineering 기법 StandardScaler() 컬럼의 개수가 너무 많다. 차원 축소 특성이 많으면 훈련 데이터에 쉽게 과대적합된다. 특성을 줄여서 학습 모델의 성능을 향상시킨다. 대표적인 방법론 중 하나가 PCA, EFA PCA vs EFA EFA(탐색적 요인 분석), Factor Analsis 예) 국어 ,수학, 과학, 영어 예 ) 국어 40, 수학 100, 과학 100, 영어 30 / 귀 학생은 언어 영역은 수준이 낮은편이나 수리영역은 매우 수준이 높습니다. 예? 번주형 &amp; 수치 데이터셋 PCA(주성분 분석) 장비1, 장비2, 장비 3, … pc1, pc2 ,pc3 … 원래 가지고 있던 정보를 알 수 없음(정보 손실) 범주형 데이터셋에는 사용 안됨 무조건 수치형 데이터에서만 사용실행 ) p.320 123# 데이터 내려받기 -O(대문자 O이다)!wget https://bit.ly/fruits_300_data -O fruits_300.npy requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); --2022-07-05 07:50:20-- https://bit.ly/fruits_300_data Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11 Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy [following] --2022-07-05 07:50:20-- https://github.com/rickiepark/hg-mldl/raw/master/fruits_300.npy Resolving github.com (github.com)... 140.82.113.4 Connecting to github.com (github.com)|140.82.113.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy [following] --2022-07-05 07:50:21-- https://raw.githubusercontent.com/rickiepark/hg-mldl/master/fruits_300.npy Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000128 (2.9M) [application/octet-stream] Saving to: ‘fruits_300.npy’ fruits_300.npy 100%[===================&gt;] 2.86M --.-KB/s in 0.02s 2022-07-05 07:50:21 (172 MB/s) - ‘fruits_300.npy’ saved [3000128/3000128] 12345import numpy as npfruits = np.load('/content/fruits_300.npy')print(fruits.shape)fruits_2d = fruits.reshape(-1,100*100) # 300개의 행, 10000개의 열fruits_2d.shape requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); (300, 100, 100) (300, 10000) PCA 1234from sklearn.decomposition import PCApca = PCA(n_components =50) #PCA 클래스의 객체를 만들 때 n_components 매개변수에 주성분 개수를 지정한다.pca.fit(fruits_2d) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); PCA(copy=True, iterated_power='auto', n_components=50, random_state=None, svd_solver='auto', tol=0.0, whiten=False) n_components를 50으로 지정해서 pca.components_ 배열의 첫번째 차원이 50으로 나타난다. 즉 50개의 주성분을 찾은 것이다. 두 번째 차원은 항상 원본 데이터의 특성 개수와 같은 10000이다. 1print(pca.components_.shape) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); (50, 10000) 1234567891011121314151617import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 1draw_fruits(pca.components_.reshape(-1,100,100)) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 123# 머신러닝에서 컬럼의 개수를 10000개에서 50개로 줄임fruits_pca = pca.transform(fruits_2d)print(fruits_pca.shape) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); (300, 50) 훈련데이터, 테스트 데이터 분리 설명된 분산 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값 1234# 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값을 설명된 분산이라고 한다.# PCA 클래스의 explained_variance_ratio_에 각 주성분의 설명된 분산 비율이 기록되어 있다. 이 분산 비율을 모두 더하면 50개의 주성분으로 표현하고 있는 총 분산 비율을 얻을 수 있다.print(np.sum(pca.explained_variance_ratio_)) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 0.9215214358477614 1234plt.plot(pca.explained_variance_ratio_)plt.show()# 그래프를 보면 처음 10개의 주성분이 대부분의 분산을 표현하고 있다. 그 다음부터의 각 주성분이 설명하는 분산은 비교적으로 작다. requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 1print(np.sum(pca.explained_variance_ratio_[:50])) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); 0.9215214358477614 12345!pip uninstall sklearn -y!pip install --upgrade sklearn!pip install scikit-learn==0.23.2 --user!pip install pycaret!pip install markupsafe==2.0.1 Found existing installation: sklearn 0.0 Uninstalling sklearn-0.0: Successfully uninstalled sklearn-0.0 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting sklearn Downloading sklearn-0.0.tar.gz (1.1 kB) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2) Requirement already satisfied: numpy&gt;=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sklearn) (1.21.6) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sklearn) (3.1.0) Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sklearn) (1.4.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sklearn) (1.1.0) Building wheels for collected packages: sklearn Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=5566fc096d6ec95daf803ef331a5510fa75a651d8e4d1d9f06b99dfd8c4e2161 Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e Successfully built sklearn Installing collected packages: sklearn Successfully installed sklearn-0.0 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting scikit-learn==0.23.2 Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) \u001b[K |████████████████████████████████| 6.8 MB 27.6 MB/s \u001b[?25hRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2) (3.1.0) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2) (1.21.6) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2) (1.1.0) Requirement already satisfied: scipy&gt;=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2) (1.4.1) Installing collected packages: scikit-learn \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. yellowbrick 1.4 requires scikit-learn&gt;=1.0.0, but you have scikit-learn 0.23.2 which is incompatible. imbalanced-learn 0.8.1 requires scikit-learn&gt;=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m Successfully installed scikit-learn-0.23.2 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting pycaret Downloading pycaret-2.3.10-py3-none-any.whl (320 kB) \u001b[K |████████████████████████████████| 320 kB 19.6 MB/s \u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.5) Collecting mlflow Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB) \u001b[K |████████████████████████████████| 17.9 MB 54.3 MB/s \u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2) Collecting pandas-profiling&gt;=2.8.0 Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB) \u001b[K |████████████████████████████████| 262 kB 42.6 MB/s \u001b[?25hRequirement already satisfied: pyyaml&lt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.13) Collecting Boruta Downloading Boruta-0.3-py3-none-any.whl (56 kB) \u001b[K |████████████████████████████████| 56 kB 4.4 MB/s \u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Collecting lightgbm&gt;=2.3.1 Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB) \u001b[K |████████████████████████████████| 2.0 MB 44.5 MB/s \u001b[?25hRequirement already satisfied: scikit-learn==0.23.2 in /root/.local/lib/python3.7/site-packages (from pycaret) (0.23.2) Requirement already satisfied: numba&lt;0.55 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.51.2) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.7) Requirement already satisfied: yellowbrick&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4) Collecting pyod Downloading pyod-1.0.2.tar.gz (122 kB) \u001b[K |████████████████████████████████| 122 kB 68.6 MB/s \u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.0) Collecting spacy&lt;2.4.0 Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB) \u001b[K |████████████████████████████████| 10.4 MB 41.6 MB/s \u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.2) Requirement already satisfied: plotly&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Requirement already satisfied: cufflinks&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3) Collecting imbalanced-learn==0.7.0 Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB) \u001b[K |████████████████████████████████| 167 kB 57.6 MB/s \u001b[?25hCollecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.20.0-py2.py3-none-any.whl (1.3 MB) \u001b[K |████████████████████████████████| 1.3 MB 53.8 MB/s \u001b[?25hCollecting kmodes&gt;=0.10.1 Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB) Requirement already satisfied: gensim&lt;4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0) Collecting pyLDAvis Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 50.2 MB/s \u001b[?25h Installing build dependencies ... \u001b[?25l\u001b[?25hdone Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone Installing backend dependencies ... \u001b[?25l\u001b[?25hdone Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone Collecting umap-learn Downloading umap-learn-0.5.3.tar.gz (88 kB) \u001b[K |████████████████████████████████| 88 kB 6.8 MB/s \u001b[?25hCollecting scikit-plot Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB) Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.7.0) Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3) Requirement already satisfied: scipy&lt;=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4.1) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0-&gt;pycaret) (1.21.6) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2-&gt;pycaret) (3.1.0) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (1.15.0) Requirement already satisfied: colorlover&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (0.3.0) Requirement already satisfied: setuptools&gt;=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (57.4.0) Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim&lt;4.0.0-&gt;pycaret) (5.2.1) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.4.2) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.8.0) Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (5.1.1) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (2.6.1) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (1.0.18) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.7.5) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.8.1) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (5.4.0) Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (3.6.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (1.1.0) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (4.10.1) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (0.2.0) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.3.5) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&gt;=2.3.1-&gt;pycaret) (0.37.1) Collecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB) \u001b[K |████████████████████████████████| 1.3 MB 63.9 MB/s \u001b[?25hRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (3.0.9) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (1.4.3) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;pycaret) (4.1.1) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.10.0) Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (2.15.3) Requirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.3.3) Requirement already satisfied: importlib-resources&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (5.7.1) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (21.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (0.18.1) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.11.4) Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.4.0-&gt;jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (3.8.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&lt;0.55-&gt;pycaret) (0.34.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;pycaret) (2022.1) Collecting markupsafe~=2.1.1 Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB) Requirement already satisfied: pydantic&gt;=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.8.2) Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (0.5.1) Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (4.64.0) Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Collecting multimethod&gt;=1.4 Downloading multimethod-1.8-py3-none-any.whl (9.8 kB) Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) \u001b[K |████████████████████████████████| 102 kB 12.2 MB/s \u001b[?25hRequirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.11.3) Collecting requests&gt;=2.24.0 Downloading requests-2.28.1-py3-none-any.whl (62 kB) \u001b[K |████████████████████████████████| 62 kB 1.6 MB/s \u001b[?25hCollecting phik&gt;=0.11.1 Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB) \u001b[K |████████████████████████████████| 690 kB 61.5 MB/s \u001b[?25hCollecting tangled-up-in-unicode==0.2.0 Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB) \u001b[K |████████████████████████████████| 4.7 MB 52.9 MB/s \u001b[?25hCollecting pyyaml&lt;6.0.0 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) \u001b[K |████████████████████████████████| 636 kB 58.4 MB/s \u001b[?25hRequirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.6.3) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (7.1.2) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) \u001b[K |████████████████████████████████| 812 kB 44.9 MB/s \u001b[?25hCollecting scipy&lt;=1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) \u001b[K |████████████████████████████████| 25.9 MB 1.3 MB/s \u001b[?25hRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly&gt;=4.4.1-&gt;pycaret) (8.0.1) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;IPython-&gt;pycaret) (0.2.5) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2022.6.15) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.10) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.7.7) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.9.1) Collecting catalogue&lt;1.1.0,&gt;=0.0.7 Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.7) Collecting srsly&lt;1.1.0,&gt;=1.0.2 Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB) \u001b[K |████████████████████████████████| 184 kB 50.9 MB/s \u001b[?25hRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (2.0.6) Collecting thinc&lt;7.5.0,&gt;=7.4.1 Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB) \u001b[K |████████████████████████████████| 1.0 MB 55.5 MB/s \u001b[?25hCollecting plac&lt;1.2.0,&gt;=0.9.6 Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (3.0.6) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.3.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.13.3) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (1.8.0) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.6.1) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (23.1.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.7.0) Collecting yellowbrick&gt;=1.0.1 Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB) \u001b[K |████████████████████████████████| 271 kB 56.5 MB/s \u001b[?25hCollecting numpy&gt;=1.13.3 Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB) \u001b[K |████████████████████████████████| 14.8 MB 71.4 MB/s \u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.3.0) Collecting docker&gt;=4.0.0 Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB) \u001b[K |████████████████████████████████| 146 kB 42.0 MB/s \u001b[?25hCollecting alembic Downloading alembic-1.8.0-py3-none-any.whl (209 kB) \u001b[K |████████████████████████████████| 209 kB 71.6 MB/s \u001b[?25hCollecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) \u001b[K |████████████████████████████████| 79 kB 342 kB/s \u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.3.0) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (3.17.3) Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4) Collecting gitpython&gt;=2.1.0 Downloading GitPython-3.1.27-py3-none-any.whl (181 kB) \u001b[K |████████████████████████████████| 181 kB 53.3 MB/s \u001b[?25hCollecting databricks-cli&gt;=0.8.7 Downloading databricks-cli-0.17.0.tar.gz (81 kB) \u001b[K |████████████████████████████████| 81 kB 10.9 MB/s \u001b[?25hCollecting querystring-parser Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB) Requirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (7.1.2) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (21.3) Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.1.4) Requirement already satisfied: sqlparse&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4.2) Requirement already satisfied: sqlalchemy&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.4.37) Collecting prometheus-flask-exporter Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB) Collecting pyjwt&gt;=1.7.0 Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB) Requirement already satisfied: oauthlib&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (3.2.0) Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (0.8.9) Collecting websocket-client&gt;=0.32.0 Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB) \u001b[K |████████████████████████████████| 54 kB 2.5 MB/s \u001b[?25hCollecting gitdb&lt;5,&gt;=4.0.1 Downloading gitdb-4.0.9-py3-none-any.whl (63 kB) \u001b[K |████████████████████████████████| 63 kB 1.8 MB/s \u001b[?25hCollecting smmap&lt;6,&gt;=3.0.1 Downloading smmap-5.0.0-py3-none-any.whl (24 kB) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.4.0-&gt;mlflow-&gt;pycaret) (1.1.2) Collecting Mako Downloading Mako-1.2.1-py3-none-any.whl (78 kB) \u001b[K |████████████████████████████████| 78 kB 7.7 MB/s \u001b[?25hRequirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.1.0) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.0.1) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (1.5.0) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.0.0) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.6.0) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.8.4) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.7.1) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.5.1) Requirement already satisfied: regex&gt;=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk-&gt;pycaret) (2022.6.2) Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter-&gt;mlflow-&gt;pycaret) (0.14.1) Collecting funcy Downloading funcy-1.17-py2.py3-none-any.whl (33 kB) Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.0) Collecting pyLDAvis Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 64.1 MB/s \u001b[?25h Installing build dependencies ... \u001b[?25l\u001b[?25hdone Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone Installing backend dependencies ... \u001b[?25l\u001b[?25hdone Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB) \u001b[K |████████████████████████████████| 1.7 MB 62.5 MB/s \u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (2.8.1) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.16.0) Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.10.2) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;pyod-&gt;pycaret) (0.5.2) Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.7.tar.gz (1.1 MB) \u001b[K |████████████████████████████████| 1.1 MB 35.1 MB/s \u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=09365b10976c7aed84ee6af9963060227fd78c4ae08a9f708f8eea72deab4a01 Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=503ea7f1d24e0c01d1b5c76e92d12d2e0430c28ca8be66599a39ea5b7f032cf2 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141960 sha256=3a4bb5b21e53e86592f100845750a1b48291ed9e6462988ca07485291daced64 Stored in directory: /root/.cache/pip/wheels/55/c3/db/33705569425fd2bdc9ea73051a8053fa26965c2bce8a146747 Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=29f3dd61970427a6fa61c6c19032aa434ac62e2870ee3392f1eb4e596c681a14 Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284 Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pyod: filename=pyod-1.0.2-py3-none-any.whl size=150272 sha256=9f4bd958fd45d14d4b55b6eec4dba2e6ac6d24a93d6742b7afc2521875af3d64 Stored in directory: /root/.cache/pip/wheels/e6/8f/06/5512935ed3c79659f612e8bb8f43cb51dd47c21973e0230997 Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=c748681d1971a7e27dcd6783b549d700f395aac60b502413f7fad6d331e9951f Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821 Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=d26ca62dd082d211384ce887c57f091d37740b9cdd75202cd25277119d17e5d1 Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent Installing collected packages: markupsafe, numpy, tangled-up-in-unicode, smmap, scipy, multimethod, websocket-client, visions, srsly, requests, pyjwt, plac, Mako, imagehash, gitdb, catalogue, thinc, querystring-parser, pyyaml, pynndescent, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, spacy, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret Attempting uninstall: markupsafe Found existing installation: MarkupSafe 2.0.1 Uninstalling MarkupSafe-2.0.1: Successfully uninstalled MarkupSafe-2.0.1 Attempting uninstall: numpy Found existing installation: numpy 1.21.6 Uninstalling numpy-1.21.6: Successfully uninstalled numpy-1.21.6 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: srsly Found existing installation: srsly 2.4.3 Uninstalling srsly-2.4.3: Successfully uninstalled srsly-2.4.3 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: catalogue Found existing installation: catalogue 2.0.7 Uninstalling catalogue-2.0.7: Successfully uninstalled catalogue-2.0.7 Attempting uninstall: thinc Found existing installation: thinc 8.0.17 Uninstalling thinc-8.0.17: Successfully uninstalled thinc-8.0.17 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: yellowbrick Found existing installation: yellowbrick 1.4 Uninstalling yellowbrick-1.4: Successfully uninstalled yellowbrick-1.4 Attempting uninstall: spacy Found existing installation: spacy 3.3.1 Uninstalling spacy-3.3.1: Successfully uninstalled spacy-3.3.1 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Attempting uninstall: mlxtend Found existing installation: mlxtend 0.14.0 Uninstalling mlxtend-0.14.0: Successfully uninstalled mlxtend-0.14.0 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 Attempting uninstall: imbalanced-learn Found existing installation: imbalanced-learn 0.8.1 Uninstalling imbalanced-learn-0.8.1: Successfully uninstalled imbalanced-learn-0.8.1 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. xarray-einstats 0.2.2 requires numpy&gt;=1.21, but you have numpy 1.19.5 which is incompatible. tensorflow 2.8.2+zzzcolab20220527125636 requires numpy&gt;=1.20, but you have numpy 1.19.5 which is incompatible. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible. en-core-web-sm 3.3.0 requires spacy&lt;3.4.0,&gt;=3.3.0.dev0, but you have spacy 2.3.7 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m Successfully installed Boruta-0.3 Mako-1.2.1 alembic-1.8.0 catalogue-1.0.0 databricks-cli-0.17.0 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 kmodes-0.12.1 lightgbm-3.3.2 markupsafe-2.1.1 mlflow-1.27.0 mlxtend-0.19.0 multimethod-1.8 numpy-1.19.5 pandas-profiling-3.2.0 phik-0.12.2 plac-1.1.3 prometheus-flask-exporter-0.20.2 pyLDAvis-3.2.2 pycaret-2.3.10 pyjwt-2.4.0 pynndescent-0.5.7 pyod-1.0.2 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.28.1 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 spacy-2.3.7 srsly-1.0.5 tangled-up-in-unicode-0.2.0 thinc-7.4.5 umap-learn-0.5.3 visions-0.7.4 websocket-client-1.3.3 yellowbrick-1.3.post1 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting markupsafe==2.0.1 Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB) Installing collected packages: markupsafe Attempting uninstall: markupsafe Found existing installation: MarkupSafe 2.1.1 Uninstalling MarkupSafe-2.1.1: Successfully uninstalled MarkupSafe-2.1.1 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pandas-profiling 3.2.0 requires markupsafe~=2.1.1, but you have markupsafe 2.0.1 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m Successfully installed markupsafe-2.0.1 12from pycaret.utils import enable_colabenable_colab() Colab mode enabled. 12from pycaret.datasets import get_datadataset = get_data('credit') requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default 0 20000 2 2 1 24 2 2 -1 -1 -2 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 90000 2 2 2 34 0 0 0 0 0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 2 50000 2 2 1 37 0 0 0 0 0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 3 50000 1 2 1 57 -1 0 -1 0 0 ... 20940.0 19146.0 19131.0 2000.0 36681.0 10000.0 9000.0 689.0 679.0 0 4 50000 1 1 2 37 0 0 0 0 0 ... 19394.0 19619.0 20024.0 2500.0 1815.0 657.0 1000.0 1000.0 800.0 0 5 rows × 24 columns &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-003d30fc-6b2a-4861-920f-6f43428d0b62 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-003d30fc-6b2a-4861-920f-6f43428d0b62'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1dataset.info() #credit에 대한 information을 가져오는 것 requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 24000 entries, 0 to 23999 Data columns (total 24 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 LIMIT_BAL 24000 non-null int64 1 SEX 24000 non-null int64 2 EDUCATION 24000 non-null int64 3 MARRIAGE 24000 non-null int64 4 AGE 24000 non-null int64 5 PAY_1 24000 non-null int64 6 PAY_2 24000 non-null int64 7 PAY_3 24000 non-null int64 8 PAY_4 24000 non-null int64 9 PAY_5 24000 non-null int64 10 PAY_6 24000 non-null int64 11 BILL_AMT1 24000 non-null float64 12 BILL_AMT2 24000 non-null float64 13 BILL_AMT3 24000 non-null float64 14 BILL_AMT4 24000 non-null float64 15 BILL_AMT5 24000 non-null float64 16 BILL_AMT6 24000 non-null float64 17 PAY_AMT1 24000 non-null float64 18 PAY_AMT2 24000 non-null float64 19 PAY_AMT3 24000 non-null float64 20 PAY_AMT4 24000 non-null float64 21 PAY_AMT5 24000 non-null float64 22 PAY_AMT6 24000 non-null float64 23 default 24000 non-null int64 dtypes: float64(12), int64(12) memory usage: 4.4 MB 12345678#credit 에 대한 데이터를 가공하는 과정data = dataset.sample(frac=0.95, random_state=786)data_unseen = dataset.drop(data.index)data.reset_index(inplace=True, drop=True)data_unseen.reset_index(inplace=True, drop=True)print('Data for Modeling: ' + str(data.shape))print('Unseen Data For Predictions: ' + str(data_unseen.shape)) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); Data for Modeling: (22800, 24) Unseen Data For Predictions: (1200, 24) set up 1234# 설정을 해주는 단계import jinja2from pycaret.classification import *exp_clf101 = setup(data = data, target = 'default', session_id=123) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Description Value 0 session_id 123 1 Target default 2 Target Type Binary 3 Label Encoded None 4 Original Data (22800, 24) 5 Missing Values False 6 Numeric Features 14 7 Categorical Features 9 8 Ordinal Features False 9 High Cardinality Features False 10 High Cardinality Method None 11 Transformed Train Set (15959, 88) 12 Transformed Test Set (6841, 88) 13 Shuffle Train-Test True 14 Stratify Train-Test False 15 Fold Generator StratifiedKFold 16 Fold Number 10 17 CPU Jobs -1 18 Use GPU False 19 Log Experiment False 20 Experiment Name clf-default-name 21 USI dac2 22 Imputation Type simple 23 Iterative Imputation Iteration None 24 Numeric Imputer mean 25 Iterative Imputation Numeric Model None 26 Categorical Imputer constant 27 Iterative Imputation Categorical Model None 28 Unknown Categoricals Handling least_frequent 29 Normalize False 30 Normalize Method None 31 Transformation False 32 Transformation Method None 33 PCA False 34 PCA Method None 35 PCA Components None 36 Ignore Low Variance False 37 Combine Rare Levels False 38 Rare Level Threshold None 39 Numeric Binning False 40 Remove Outliers False 41 Outliers Threshold None 42 Remove Multicollinearity False 43 Multicollinearity Threshold None 44 Remove Perfect Collinearity True 45 Clustering False 46 Clustering Iteration None 47 Polynomial Features False 48 Polynomial Degree None 49 Trignometry Features False 50 Polynomial Threshold None 51 Group Features False 52 Feature Selection False 53 Feature Selection Method classic 54 Features Selection Threshold None 55 Feature Interaction False 56 Feature Ratio False 57 Interaction Threshold None 58 Fix Imbalance False 59 Fix Imbalance Method SMOTE &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-af499831-4c50-410c-b849-a648fea8cc27 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-af499831-4c50-410c-b849-a648fea8cc27'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 모델링1best_model = compare_models() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) ridge Ridge Classifier 0.8254 0.0000 0.3637 0.6913 0.4764 0.3836 0.4122 0.050 lda Linear Discriminant Analysis 0.8247 0.7634 0.3755 0.6794 0.4835 0.3884 0.4132 0.300 gbc Gradient Boosting Classifier 0.8226 0.7789 0.3551 0.6806 0.4664 0.3725 0.4010 5.809 ada Ada Boost Classifier 0.8221 0.7697 0.3505 0.6811 0.4626 0.3690 0.3983 1.306 lightgbm Light Gradient Boosting Machine 0.8210 0.7750 0.3609 0.6679 0.4683 0.3721 0.3977 0.408 rf Random Forest Classifier 0.8199 0.7598 0.3663 0.6601 0.4707 0.3727 0.3965 3.321 et Extra Trees Classifier 0.8092 0.7377 0.3677 0.6047 0.4571 0.3497 0.3657 2.400 lr Logistic Regression 0.7814 0.6410 0.0003 0.1000 0.0006 0.0003 0.0034 0.985 dummy Dummy Classifier 0.7814 0.5000 0.0000 0.0000 0.0000 0.0000 0.0000 0.031 knn K Neighbors Classifier 0.7547 0.5939 0.1763 0.3719 0.2388 0.1145 0.1259 0.986 dt Decision Tree Classifier 0.7293 0.6147 0.4104 0.3878 0.3986 0.2242 0.2245 0.376 svm SVM - Linear Kernel 0.7277 0.0000 0.1017 0.1671 0.0984 0.0067 0.0075 0.585 qda Quadratic Discriminant Analysis 0.5098 0.5473 0.6141 0.2472 0.3488 0.0600 0.0805 0.155 nb Naive Bayes 0.3760 0.6442 0.8845 0.2441 0.3826 0.0608 0.1207 0.048 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-7619a723-c12b-4142-920c-02f2ec654efd button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-7619a723-c12b-4142-920c-02f2ec654efd'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 가장 좋은 모델을 뽑아주세요 1print(best_model) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=123, solver='auto', tol=0.001) 모델 생성 12# knn 모델에 대해서knn_model = create_model('knn') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accuracy AUC Recall Prec. F1 Kappa MCC Fold 0 0.7469 0.6020 0.1920 0.3545 0.2491 0.1128 0.1204 1 0.7550 0.5894 0.2092 0.3883 0.2719 0.1402 0.1500 2 0.7506 0.5883 0.1576 0.3459 0.2165 0.0923 0.1024 3 0.7419 0.5818 0.1519 0.3136 0.2046 0.0723 0.0790 4 0.7563 0.5908 0.1490 0.3611 0.2110 0.0954 0.1085 5 0.7550 0.5997 0.1748 0.3720 0.2378 0.1139 0.1255 6 0.7638 0.5890 0.1891 0.4125 0.2593 0.1413 0.1565 7 0.7613 0.6240 0.1633 0.3904 0.2303 0.1163 0.1318 8 0.7619 0.5988 0.1862 0.4037 0.2549 0.1356 0.1500 9 0.7549 0.5756 0.1897 0.3771 0.2524 0.1246 0.1351 Mean 0.7547 0.5939 0.1763 0.3719 0.2388 0.1145 0.1259 Std 0.0065 0.0126 0.0191 0.0279 0.0214 0.0214 0.0230 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-10ece260-6a01-4354-a08f-172662845ac6 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-10ece260-6a01-4354-a08f-172662845ac6'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 123456import numpy as npparams ={ 'n_neighbors': np.arange(0,50,1)}tunned_knn = tune_model(knn_model, custom_grid =params)print(tunned_knn) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Accuracy AUC Recall Prec. F1 Kappa MCC Fold 0 0.7813 0.6482 0.0372 0.5000 0.0693 0.0402 0.0876 1 0.7807 0.6436 0.0315 0.4783 0.0591 0.0330 0.0759 2 0.7744 0.6563 0.0315 0.3333 0.0576 0.0206 0.0403 3 0.7845 0.6589 0.0659 0.5610 0.1179 0.0754 0.1345 4 0.7826 0.6645 0.0315 0.5500 0.0596 0.0368 0.0903 5 0.7794 0.6477 0.0544 0.4634 0.0974 0.0539 0.0961 6 0.7826 0.6278 0.0630 0.5238 0.1125 0.0688 0.1214 7 0.7751 0.6702 0.0372 0.3611 0.0675 0.0278 0.0523 8 0.7813 0.6409 0.0630 0.5000 0.1120 0.0662 0.1146 9 0.7881 0.6426 0.0661 0.6389 0.1198 0.0822 0.1548 Mean 0.7810 0.6501 0.0482 0.4910 0.0873 0.0505 0.0968 Std 0.0039 0.0119 0.0148 0.0861 0.0255 0.0206 0.0338 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-1bde01f8-f982-4e15-b24d-24767759a45b button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-1bde01f8-f982-4e15-b24d-24767759a45b'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=-1, n_neighbors=42, p=2, weights='uniform') auc 최소 0.5 좋은 모델 0.8 최고 1 1plot_model(tunned_knn,plot='auc') 12# 의사결정틜evaluate_model(tunned_knn) requirejs.config({ paths: { base: '/static/base', plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', }, }); interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Hyperparameters', 'param… 1","link":"/2022/07/05/day0705/"},{"title":"","text":"데이터 분석 (머신러닝, 딥러닝) 프로세스 데이터 불러오기 CSV, 오라클, MySQL, PostgreSQL, 클라우드 DB 연동 탐색적 자료 분석 데이터 전처리 및 가공 잠정적인 컬럼의 개수를 지정해야 함. 머신러닝 모델(= 통계 모델링,t.test, 분산분석, 교차분석) 머신러닝 모델의 경우 배포(X) JSP 웹 개발 시 배우게 됨 통계 모델링 경우, p-value 값 기준으로, 귀무가설 및 대립가설 검정 (공통) 결과 보고서를 작성해야함 PPT 만들어야 함. 그래프 복습 수치형 데이터 시각화 번주형 데이터 시각화 데이터 관계 시각화 matplotlib 라이브러리 방법 seaborn 라이브러리 방법 복잡한 그래프 그려야지=&gt; matplotlib 1줄 그래프 -&gt; seaborn 수치형 데이터 시각화123import seaborn as snstitanic = sns.load_dataset('titanic')titanic.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-6a537552-ed22-48df-895f-bbff39f7fece button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-6a537552-ed22-48df-895f-bbff39f7fece'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# 히스토그램sns.histplot(data = titanic, x='age',bins=10,hue = 'alive')#bins= 간격 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe55f3d7490&gt; 123# 커널밀도추정 함수 그래프# 연속형 데이터 1개만 쓸 때 사용sns.kdeplot(data = titanic, x ='age',hue = 'alive',multiple='stack') &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe55652eb90&gt; 분포도 수치형 데이터 한 개 컬럼의 분포를 나타내는 그래프 정규분포인가? 1sns.displot(data = titanic, x='age') &lt;seaborn.axisgrid.FacetGrid at 0x7fe561559290&gt; 1sns.displot(titanic, x='age', kde='True') &lt;seaborn.axisgrid.FacetGrid at 0x7fe561417bd0&gt; 범주형 데이터 시각화 x축 범주형, y축 수치 데이터 x축 범주형, y축 범주형 히트맵 12# 막대 그래프sns.barplot(x='class', y='fare', data=titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5613316d0&gt; 12# 포인트 플롯sns.pointplot(x='class', y='fare', data = titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5564ac750&gt; boxplot 제 1사분위: 전체 데이터 중 하위 25% 사분위 범위 수 (IQR) : 제 3 사분위 - 제 1사분위 최댓값: 제 3사분위 +(1.5 * IQR) 12# boxplotsns.boxplot(x='class', y='age', data =titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe561243e90&gt; 12# 바이올린 플롯sns.violinplot(x='class', y='age', hue='sex',data=titanic, split = True); 카운트 플롯 범주형 데이터의 개수 확인 할 때 사용 1sns.countplot(x='class',hue='alive', data = titanic) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe561078790&gt; 데이터 관계 시각화 여러 데이터 사이의 관계도 파악 위한 그래프 123# 히트맵flights = sns.load_dataset('flights')flights.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month passengers 0 1949 Jan 112 1 1949 Feb 118 2 1949 Mar 132 3 1949 Apr 129 4 1949 May 121 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-7799d7e2-5544-4e3e-a632-07ef23100543 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-7799d7e2-5544-4e3e-a632-07ef23100543'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1flights['year'].value_counts() 1949 12 1950 12 1951 12 1952 12 1953 12 1954 12 1955 12 1956 12 1957 12 1958 12 1959 12 1960 12 Name: year, dtype: int64 1flights.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 144 entries, 0 to 143 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 year 144 non-null int64 1 month 144 non-null category 2 passengers 144 non-null int64 dtypes: category(1), int64(2) memory usage: 2.9 KB pivot 사용방법: 세로, 가로그리고 결과값으로 바꿀 수 있다. 123import pandas as pdfrom pandas import Series, DataFrame 12flights_pivot=flights.pivot(index='month',columns='year',values='passengers')flights_pivot .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 month Jan 112 115 145 171 196 204 242 284 315 340 360 417 Feb 118 126 150 180 196 188 233 277 301 318 342 391 Mar 132 141 178 193 236 235 267 317 356 362 406 419 Apr 129 135 163 181 235 227 269 313 348 348 396 461 May 121 125 172 183 229 234 270 318 355 363 420 472 Jun 135 149 178 218 243 264 315 374 422 435 472 535 Jul 148 170 199 230 264 302 364 413 465 491 548 622 Aug 148 170 199 242 272 293 347 405 467 505 559 606 Sep 136 158 184 209 237 259 312 355 404 404 463 508 Oct 119 133 162 191 211 229 274 306 347 359 407 461 Nov 104 114 146 172 180 203 237 271 305 310 362 390 Dec 118 140 166 194 201 229 278 306 336 337 405 432 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-637ba909-eb62-407f-8194-c6277fd1ef2e button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-637ba909-eb62-407f-8194-c6277fd1ef2e'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 1sns.heatmap(data= flights_pivot) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe56113b910&gt; 라인플롯12sns.lineplot(x ='year', y='passengers', data=flights)# 신뢰구간이 있다 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe561bba210&gt; 123# 산점도tips = sns.load_dataset('tips')tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 &lt;svg xmlns=”http://www.w3.org/2000/svg&quot; height=”24px”viewBox=”0 0 24 24” width=”24px”&gt; .colab-df-container { display:flex; flex-wrap:wrap; gap: 12px; } .colab-df-convert { background-color: #E8F0FE; border: none; border-radius: 50%; cursor: pointer; display: none; fill: #1967D2; height: 32px; padding: 0 0 0 0; width: 32px; } .colab-df-convert:hover { background-color: #E2EBFA; box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15); fill: #174EA6; } [theme=dark] .colab-df-convert { background-color: #3B4455; fill: #D2E3FC; } [theme=dark] .colab-df-convert:hover { background-color: #434B5C; box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3)); fill: #FFFFFF; } &lt;script&gt; const buttonEl = document.querySelector('#df-343bc603-e75e-4b65-8197-6aee49fba9f7 button.colab-df-convert'); buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none'; async function convertToInteractive(key) { const element = document.querySelector('#df-343bc603-e75e-4b65-8197-6aee49fba9f7'); const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {}); if (!dataTable) return; const docLinkHtml = 'Like what you see? Visit the ' + '&lt;a target=&quot;_blank&quot; href=https://colab.research.google.com/notebooks/data_table.ipynb&gt;data table notebook&lt;/a&gt;' + ' to learn more about interactive tables.'; element.innerHTML = ''; dataTable['output_type'] = 'display_data'; await google.colab.output.renderOutput(dataTable, element); const docLink = document.createElement('div'); docLink.innerHTML = docLinkHtml; element.appendChild(docLink); } &lt;/script&gt; &lt;/div&gt; 12# 두 개의 연속형 데이터sns.scatterplot(x='total_bill', y='tip',hue='time',style='sex',data = tips) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5564868d0&gt; 12# 회귀선sns.regplot(x='total_bill', y='tip', data= tips) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5563aa810&gt; 선형 회귀 선형 회귀식을 찾는 것이 중요 $y = 3x + 4$에 근사한 데이터 50개 생성 12345678910111213141516import numpy as np import pandas as pd# 시드값 고정 np.random.seed(0)intercept = 4 # 절편, 상수slope = 3 # 기울기# 변동성 주기 위해 노이즈 생성noise = np.random.randn(50, 1)x = 5 * np.random.rand(50, 1) # 0과 5사이의 실숫값 50개 생성y = slope * x + intercept + noise# 데이터 프레임 생성data = pd.DataFrame({'X' : x[:, 0], 'Y' : y[:, 0]})print(data) X Y 0 0.794848 8.148596 1 0.551876 6.055784 2 3.281648 14.823682 3 0.690915 8.313637 4 0.982912 8.816293 5 1.843626 8.553600 6 4.104966 17.264987 7 0.485506 5.305162 8 4.189725 16.465955 9 0.480492 5.852075 10 4.882297 18.790936 11 2.343256 12.484042 12 4.883805 19.412454 13 3.024228 13.194358 14 3.696318 15.532817 15 0.195939 4.921491 16 1.414035 9.736184 17 0.600983 5.597790 18 1.480701 8.755171 19 0.593639 4.926820 20 1.589916 6.216758 21 2.071315 10.867564 22 0.320737 5.826649 23 3.462361 13.644917 24 2.833007 14.768776 25 1.326947 6.526477 26 2.616240 11.894479 27 0.469703 5.221924 28 2.879732 14.171977 29 4.646481 19.408802 30 1.592845 8.933482 31 3.337052 14.389318 32 0.658989 5.089182 33 3.581636 12.764112 34 1.447030 7.993179 35 0.915957 6.904219 36 2.932565 14.027985 37 0.100538 5.503993 38 4.144700 16.046774 39 0.023477 3.768129 40 3.389083 13.118695 41 1.350040 6.630102 42 3.675970 13.321640 43 4.810943 20.383604 44 1.243766 7.221645 45 2.880787 12.204286 46 2.960210 11.627834 47 2.861260 13.361269 48 1.115408 5.732327 49 4.763745 18.078495 12345import matplotlib.pyplot as pltfig, ax = plt.subplots()ax.scatter(data['X'], data['Y'])plt.show() 12import seaborn as sns sns.scatterplot(x = 'X', y = 'Y', data = data) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe5562eef90&gt; 선형 회귀 모형 훈련 모형 생성 후, 회귀 계수 3과 y 절편 4에 근사한 값이 나와야 한다. 123456from sklearn.linear_model import LinearRegression lr_model = LinearRegression() # 선형 회귀 모델 lr_model.fit(x, y) # 모델 훈련print('y절편:', lr_model.intercept_)print('회귀계수:', lr_model.coef_) y절편: [4.05757639] 회귀계수: [[3.03754061]] 1234567891011# 예측값y_pred = lr_model.predict(x)fig, ax = plt.subplots()ax.scatter(x, y)ax.plot(x, y_pred, color='green')# slope, intercept label = 'slope: {}\\nintercept: {}'.format(round(lr_model.coef_[0][0], 2), round(lr_model.intercept_[0], 2))ax.text(3.5, 4, label, style ='italic', fontsize = 10, color =&quot;green&quot;)plt.show() 로지스틱 회귀1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltdef sigmoid(arr, scale=1): arr = np.asarray(arr) result = 1/(1 + np.exp(-arr*scale)) return resultx = np.linspace(-6, 6)y = sigmoid(x)fig, ax = plt.subplots()ax.plot(x, y)ax.grid(which='major', axis='y', linestyle='--')ax.axvline(x=0, color='r', linestyle='--', linewidth=1)ax.set_ylim(0,1)ax.set_yticks([0, 1, 0.5])ax.text(0-0.1, 0.5, '0.5', ha='right')ax.set_title('Sigmoid Graph')plt.show() 12345678910111213# 라이브러리 불러오기import matplotlib.pyplot as pltimport numpy as npfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import classification_report, confusion_matrix# 데이터 가져오기x = np.arange(10).reshape(-1, 1)y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])# 모델 생성 및 학습model = LogisticRegression(solver='liblinear', C=10.0, random_state=0)model.fit(x, y) LogisticRegression(C=10.0, random_state=0, solver='liblinear') 모형 평가12p_pred = model.predict_proba(x)print('p_pred', p_pred, sep='\\n') p_pred [[0.97979027 0.02020973] [0.94958202 0.05041798] [0.87976149 0.12023851] [0.73975066 0.26024934] [0.52477284 0.47522716] [0.30020373 0.69979627] [0.1428487 0.8571513 ] [0.06080627 0.93919373] [0.02453462 0.97546538] [0.00967652 0.99032348]] 12y_pred = model.predict(x)print('y_pred',y_pred) y_pred [0 0 0 0 0 1 1 1 1 1] 12345678910fig, ax = plt.subplots()ax.scatter(x, y)ax.plot(x, p_pred[:, 1], color = 'black', marker='o', markersize=6)ax.plot()ax.set_xticks(x)ax.set_yticks(np.arange(0, 1.1, 0.1))ax.grid(which='major', alpha=0.5)plt.show() 12conf_m = confusion_matrix(y, y_pred)print(conf_m) [[5 0] [0 5]] 123456789101112cm = confusion_matrix(y, y_pred)fig, ax = plt.subplots(figsize=(8, 8))ax.imshow(cm, cmap = 'Pastel2')ax.grid(False)ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0', 'Predicted 1'))ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0', 'Actual 1'))ax.set_ylim(1.5, -0.5)for i in range(2): for j in range(2): ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontsize=40)plt.show() 결정 트리 분류와 회귀 문제에 모두 사용 가능 주요 개념 작동 원리 데이터를 가장 잘 구분하는 조건을 정함. 조건을 기준으로 데이터를 두 범주로 나눔 나뉜 각 범주의 데이터를 구분하는 조건을 정함 각 조건을 기준으로 데이터를 두 범주로 나눔 언제까지 계속 분할할지 정한 후, 최종 결정 값을 구함. 불순도(Impurity) 한 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지 나타냄 흰색과 검은색이 50:50으로 섞여 있다. (불순도 최대) 흰색과 검은색으로 완전 분리 되었다. (불순도 최소) 엔트로피(Entropy) 불확실한 정도를 의미함. 0 ~ 1로 정함. 흰색과 검은색이 50:50으로 섞여 있다. 엔트로피 1 흰색과 검은색으로 완전 분리 되었다. 엔트로피 0 정보이득(Information Gain) 1에서 엔트로피를 뺀 수치 정보 이득을 최대화하는 방향(엔트로피를 최소화 하는 방향)으로 노드를 분할함 지니 불순도(Gini Impurity) 지니 불순도 값이 클수록 불순도도 높고, 작을수록 불순도도 낮음. 엔트로피와 마찬가지로 지니 불순도가 낮아지는 방향으로 노드 분할함. 1234567from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_split import seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')titanic.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 survived 891 non-null int64 1 pclass 891 non-null int64 2 sex 891 non-null object 3 age 714 non-null float64 4 sibsp 891 non-null int64 5 parch 891 non-null int64 6 fare 891 non-null float64 7 embarked 889 non-null object 8 class 891 non-null category 9 who 891 non-null object 10 adult_male 891 non-null bool 11 deck 203 non-null category 12 embark_town 889 non-null object 13 alive 891 non-null object 14 alone 891 non-null bool dtypes: bool(2), category(2), float64(2), int64(4), object(5) memory usage: 80.7+ KB -survived의 비율을 구한다.0: 사망자1: 생존자 1titanic['survived'].value_counts() 0 549 1 342 Name: survived, dtype: int64 1234567# 데이터 추출X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)X_train.shape, X_test.shape, y_train.shape, y_test.shape ((623, 3), (268, 3), (623,), (268,)) 12345tree_model = DecisionTreeClassifier()tree_model.fit(X_train, y_train)acc = tree_model.score(X_test, y_test)print(f'모형 정확도 : {acc:.3f}') # 정확도 측정 모형 정확도 : 0.675 랜덤포레스12345678910111213141516171819from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_split import seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)# 모델 훈련rf_model = RandomForestClassifier(random_state=42) # 랜덤 포레스트 정의rf_model.fit(X_train, y_train)acc = tree_model.score(X_test, y_test)print(f'모형 정확도 : {acc:.3f}') # 정확도 측정 ㅠ 모형 정확도 : 0.675 XGBoost&amp; LightGBM 전통적인 머신러닝 알고리즘의 융합 선형회귀 릿지 라쏘, 과적합 방지 위한 규제 결정 트리의 핵심적인 알고리즘 경사 하강법 부스팅 기법 문제점: 파라미터의 개수가 매우 많음 왜 많이 쓸까? 모델 학습 속도 성능 가장 좋은 모델이란, 학습 속도는 빠르면서 성능은 좋은 것(지금까지 나온 알고리즘 보다) Python JAVA, C, C++ C, C++ 첫번째 옵션, 우리가 자체적으로 배포하자 -&gt; Python Wrapper API|-R, 머신러닝 프레임워크 종류 다양 두번째 옵션 파이썬 머신러닝 = Scikit_Learn에서 쉽게 쓸 수 있도록 개발, Scikit-Learn Wrapper APi 1234567891011121314151617181920import xgboost as xgb from sklearn.model_selection import train_test_splitimport seaborn as sns # 데이터 분리titanic = sns.load_dataset('titanic')# titanic.info()# X, 독립변수, y 종속변수X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)X_train.shape, X_test.shape, y_train.shape, y_test.shape ((623, 3), (268, 3), (623,), (268,)) 여기가 핵심 1234dtrain = xgb.DMatrix(data = X_train, label = y_train)dtest = xgb.DMatrix(data = X_test,label = y_test)print(dtrain) &lt;xgboost.core.DMatrix object at 0x7fe5609ac190&gt; 머신러닝 코드 1234567891011121314params = { 'max_depth':3, 'n_estimators':100, 'eta':0.1, 'objective':'binary:logistic' }num_rounds = 400w_list =[(dtrain,'train'),(dtest,'test')]xgb_ml = xgb.train(params = params, dtrain=dtrain, num_boost_round=400, early_stopping_rounds =100, evals= w_list) [0] train-error:0.260032 test-error:0.302239 Multiple eval metrics have been passed: 'test-error' will be used for early stopping. Will train until test-error hasn't improved in 100 rounds. [1] train-error:0.260032 test-error:0.302239 [2] train-error:0.260032 test-error:0.302239 [3] train-error:0.260032 test-error:0.302239 [4] train-error:0.260032 test-error:0.302239 [5] train-error:0.260032 test-error:0.302239 [6] train-error:0.260032 test-error:0.302239 [7] train-error:0.260032 test-error:0.302239 [8] train-error:0.260032 test-error:0.302239 [9] train-error:0.260032 test-error:0.302239 [10] train-error:0.260032 test-error:0.302239 [11] train-error:0.260032 test-error:0.302239 [12] train-error:0.260032 test-error:0.302239 [13] train-error:0.247191 test-error:0.298507 [14] train-error:0.247191 test-error:0.298507 [15] train-error:0.248796 test-error:0.302239 [16] train-error:0.248796 test-error:0.302239 [17] train-error:0.248796 test-error:0.302239 [18] train-error:0.248796 test-error:0.302239 [19] train-error:0.248796 test-error:0.302239 [20] train-error:0.248796 test-error:0.302239 [21] train-error:0.248796 test-error:0.302239 [22] train-error:0.248796 test-error:0.302239 [23] train-error:0.248796 test-error:0.302239 [24] train-error:0.248796 test-error:0.302239 [25] train-error:0.248796 test-error:0.302239 [26] train-error:0.248796 test-error:0.302239 [27] train-error:0.248796 test-error:0.302239 [28] train-error:0.247191 test-error:0.302239 [29] train-error:0.247191 test-error:0.302239 [30] train-error:0.247191 test-error:0.302239 [31] train-error:0.243981 test-error:0.298507 [32] train-error:0.247191 test-error:0.302239 [33] train-error:0.243981 test-error:0.298507 [34] train-error:0.243981 test-error:0.298507 [35] train-error:0.242376 test-error:0.294776 [36] train-error:0.24077 test-error:0.294776 [37] train-error:0.24077 test-error:0.294776 [38] train-error:0.24077 test-error:0.294776 [39] train-error:0.24077 test-error:0.294776 [40] train-error:0.24077 test-error:0.294776 [41] train-error:0.24077 test-error:0.294776 [42] train-error:0.24077 test-error:0.294776 [43] train-error:0.24077 test-error:0.294776 [44] train-error:0.24077 test-error:0.302239 [45] train-error:0.24077 test-error:0.302239 [46] train-error:0.24077 test-error:0.302239 [47] train-error:0.24077 test-error:0.302239 [48] train-error:0.24077 test-error:0.302239 [49] train-error:0.24077 test-error:0.302239 [50] train-error:0.24077 test-error:0.302239 [51] train-error:0.24077 test-error:0.302239 [52] train-error:0.23435 test-error:0.302239 [53] train-error:0.23435 test-error:0.302239 [54] train-error:0.232745 test-error:0.298507 [55] train-error:0.229535 test-error:0.298507 [56] train-error:0.229535 test-error:0.298507 [57] train-error:0.229535 test-error:0.298507 [58] train-error:0.229535 test-error:0.298507 [59] train-error:0.227929 test-error:0.294776 [60] train-error:0.227929 test-error:0.298507 [61] train-error:0.227929 test-error:0.298507 [62] train-error:0.227929 test-error:0.298507 [63] train-error:0.227929 test-error:0.298507 [64] train-error:0.227929 test-error:0.298507 [65] train-error:0.227929 test-error:0.298507 [66] train-error:0.227929 test-error:0.298507 [67] train-error:0.227929 test-error:0.298507 [68] train-error:0.227929 test-error:0.298507 [69] train-error:0.227929 test-error:0.298507 [70] train-error:0.227929 test-error:0.298507 [71] train-error:0.227929 test-error:0.298507 [72] train-error:0.227929 test-error:0.302239 [73] train-error:0.227929 test-error:0.302239 [74] train-error:0.229535 test-error:0.30597 [75] train-error:0.229535 test-error:0.30597 [76] train-error:0.229535 test-error:0.30597 [77] train-error:0.229535 test-error:0.30597 [78] train-error:0.229535 test-error:0.30597 [79] train-error:0.229535 test-error:0.30597 [80] train-error:0.229535 test-error:0.30597 [81] train-error:0.229535 test-error:0.30597 [82] train-error:0.229535 test-error:0.30597 [83] train-error:0.229535 test-error:0.30597 [84] train-error:0.229535 test-error:0.30597 [85] train-error:0.229535 test-error:0.30597 [86] train-error:0.229535 test-error:0.30597 [87] train-error:0.229535 test-error:0.30597 [88] train-error:0.229535 test-error:0.30597 [89] train-error:0.229535 test-error:0.30597 [90] train-error:0.229535 test-error:0.30597 [91] train-error:0.229535 test-error:0.30597 [92] train-error:0.229535 test-error:0.30597 [93] train-error:0.229535 test-error:0.30597 [94] train-error:0.227929 test-error:0.313433 [95] train-error:0.226324 test-error:0.313433 [96] train-error:0.223114 test-error:0.317164 [97] train-error:0.223114 test-error:0.317164 [98] train-error:0.223114 test-error:0.317164 [99] train-error:0.223114 test-error:0.317164 [100] train-error:0.223114 test-error:0.317164 [101] train-error:0.223114 test-error:0.317164 [102] train-error:0.223114 test-error:0.317164 [103] train-error:0.223114 test-error:0.317164 [104] train-error:0.223114 test-error:0.317164 [105] train-error:0.223114 test-error:0.317164 [106] train-error:0.223114 test-error:0.317164 [107] train-error:0.223114 test-error:0.317164 [108] train-error:0.223114 test-error:0.317164 [109] train-error:0.223114 test-error:0.317164 [110] train-error:0.223114 test-error:0.317164 [111] train-error:0.223114 test-error:0.317164 [112] train-error:0.223114 test-error:0.317164 [113] train-error:0.223114 test-error:0.317164 [114] train-error:0.223114 test-error:0.317164 [115] train-error:0.223114 test-error:0.317164 [116] train-error:0.223114 test-error:0.317164 [117] train-error:0.223114 test-error:0.317164 [118] train-error:0.223114 test-error:0.317164 [119] train-error:0.223114 test-error:0.317164 [120] train-error:0.223114 test-error:0.317164 [121] train-error:0.223114 test-error:0.317164 [122] train-error:0.223114 test-error:0.317164 [123] train-error:0.223114 test-error:0.317164 [124] train-error:0.224719 test-error:0.317164 [125] train-error:0.224719 test-error:0.317164 [126] train-error:0.224719 test-error:0.317164 [127] train-error:0.221509 test-error:0.317164 [128] train-error:0.223114 test-error:0.317164 [129] train-error:0.219904 test-error:0.313433 [130] train-error:0.215088 test-error:0.313433 [131] train-error:0.215088 test-error:0.313433 [132] train-error:0.215088 test-error:0.313433 [133] train-error:0.215088 test-error:0.313433 [134] train-error:0.215088 test-error:0.313433 [135] train-error:0.215088 test-error:0.313433 Stopping. Best iteration: [35] train-error:0.242376 test-error:0.294776 12345from sklearn.metrics import accuracy_scorepred_probs =xgb_ml.predict(dtest)y_pred = [1 if x&gt;0.5 else 0 for x in pred_probs]accuracy_score(y_pred,y_test) 0.6977611940298507 1#scikit-learn api 방식 1234567891011121314151617from sklearn.tree import DecisionTreeClassifierfrom xgboost import XGBClassifier #API방식# dt = DecisionTreeClassifier()xgb_model = XGBClassifier(objective = 'binary:logistic', max_depth=3, learning_rate = 0.1, n_estimators=100, random_state = 42)w_list = [(X_train,y_train),(X_test,y_test)]xgb_model.fit(X_train,y_train,eval_set =w_list, eval_metric='error',verbose=True)y_probas = xgb_model.predict_proba(X_test)y_pred = [1 if x&gt;0.5 else 0 for x in pred_probs]accuracy_score(y_pred,y_test) [0] validation_0-error:0.260032 validation_1-error:0.302239 [1] validation_0-error:0.260032 validation_1-error:0.302239 [2] validation_0-error:0.260032 validation_1-error:0.302239 [3] validation_0-error:0.260032 validation_1-error:0.302239 [4] validation_0-error:0.260032 validation_1-error:0.302239 [5] validation_0-error:0.260032 validation_1-error:0.302239 [6] validation_0-error:0.260032 validation_1-error:0.302239 [7] validation_0-error:0.260032 validation_1-error:0.302239 [8] validation_0-error:0.260032 validation_1-error:0.302239 [9] validation_0-error:0.260032 validation_1-error:0.302239 [10] validation_0-error:0.260032 validation_1-error:0.302239 [11] validation_0-error:0.260032 validation_1-error:0.302239 [12] validation_0-error:0.260032 validation_1-error:0.302239 [13] validation_0-error:0.247191 validation_1-error:0.298507 [14] validation_0-error:0.247191 validation_1-error:0.298507 [15] validation_0-error:0.248796 validation_1-error:0.302239 [16] validation_0-error:0.248796 validation_1-error:0.302239 [17] validation_0-error:0.248796 validation_1-error:0.302239 [18] validation_0-error:0.248796 validation_1-error:0.302239 [19] validation_0-error:0.248796 validation_1-error:0.302239 [20] validation_0-error:0.248796 validation_1-error:0.302239 [21] validation_0-error:0.248796 validation_1-error:0.302239 [22] validation_0-error:0.248796 validation_1-error:0.302239 [23] validation_0-error:0.248796 validation_1-error:0.302239 [24] validation_0-error:0.248796 validation_1-error:0.302239 [25] validation_0-error:0.248796 validation_1-error:0.302239 [26] validation_0-error:0.248796 validation_1-error:0.302239 [27] validation_0-error:0.248796 validation_1-error:0.302239 [28] validation_0-error:0.247191 validation_1-error:0.302239 [29] validation_0-error:0.247191 validation_1-error:0.302239 [30] validation_0-error:0.247191 validation_1-error:0.302239 [31] validation_0-error:0.243981 validation_1-error:0.298507 [32] validation_0-error:0.247191 validation_1-error:0.302239 [33] validation_0-error:0.243981 validation_1-error:0.298507 [34] validation_0-error:0.243981 validation_1-error:0.298507 [35] validation_0-error:0.242376 validation_1-error:0.294776 [36] validation_0-error:0.24077 validation_1-error:0.294776 [37] validation_0-error:0.24077 validation_1-error:0.294776 [38] validation_0-error:0.24077 validation_1-error:0.294776 [39] validation_0-error:0.24077 validation_1-error:0.294776 [40] validation_0-error:0.24077 validation_1-error:0.294776 [41] validation_0-error:0.24077 validation_1-error:0.294776 [42] validation_0-error:0.24077 validation_1-error:0.294776 [43] validation_0-error:0.24077 validation_1-error:0.294776 [44] validation_0-error:0.24077 validation_1-error:0.302239 [45] validation_0-error:0.24077 validation_1-error:0.302239 [46] validation_0-error:0.24077 validation_1-error:0.302239 [47] validation_0-error:0.24077 validation_1-error:0.302239 [48] validation_0-error:0.24077 validation_1-error:0.302239 [49] validation_0-error:0.24077 validation_1-error:0.302239 [50] validation_0-error:0.24077 validation_1-error:0.302239 [51] validation_0-error:0.24077 validation_1-error:0.302239 [52] validation_0-error:0.23435 validation_1-error:0.302239 [53] validation_0-error:0.23435 validation_1-error:0.302239 [54] validation_0-error:0.232745 validation_1-error:0.298507 [55] validation_0-error:0.229535 validation_1-error:0.298507 [56] validation_0-error:0.229535 validation_1-error:0.298507 [57] validation_0-error:0.229535 validation_1-error:0.298507 [58] validation_0-error:0.229535 validation_1-error:0.298507 [59] validation_0-error:0.227929 validation_1-error:0.294776 [60] validation_0-error:0.227929 validation_1-error:0.298507 [61] validation_0-error:0.227929 validation_1-error:0.298507 [62] validation_0-error:0.227929 validation_1-error:0.298507 [63] validation_0-error:0.227929 validation_1-error:0.298507 [64] validation_0-error:0.227929 validation_1-error:0.298507 [65] validation_0-error:0.227929 validation_1-error:0.298507 [66] validation_0-error:0.227929 validation_1-error:0.298507 [67] validation_0-error:0.227929 validation_1-error:0.298507 [68] validation_0-error:0.227929 validation_1-error:0.298507 [69] validation_0-error:0.227929 validation_1-error:0.298507 [70] validation_0-error:0.227929 validation_1-error:0.298507 [71] validation_0-error:0.227929 validation_1-error:0.298507 [72] validation_0-error:0.227929 validation_1-error:0.302239 [73] validation_0-error:0.227929 validation_1-error:0.302239 [74] validation_0-error:0.229535 validation_1-error:0.30597 [75] validation_0-error:0.229535 validation_1-error:0.30597 [76] validation_0-error:0.229535 validation_1-error:0.30597 [77] validation_0-error:0.229535 validation_1-error:0.30597 [78] validation_0-error:0.229535 validation_1-error:0.30597 [79] validation_0-error:0.229535 validation_1-error:0.30597 [80] validation_0-error:0.229535 validation_1-error:0.30597 [81] validation_0-error:0.229535 validation_1-error:0.30597 [82] validation_0-error:0.229535 validation_1-error:0.30597 [83] validation_0-error:0.229535 validation_1-error:0.30597 [84] validation_0-error:0.229535 validation_1-error:0.30597 [85] validation_0-error:0.229535 validation_1-error:0.30597 [86] validation_0-error:0.229535 validation_1-error:0.30597 [87] validation_0-error:0.229535 validation_1-error:0.30597 [88] validation_0-error:0.229535 validation_1-error:0.30597 [89] validation_0-error:0.229535 validation_1-error:0.30597 [90] validation_0-error:0.229535 validation_1-error:0.30597 [91] validation_0-error:0.229535 validation_1-error:0.30597 [92] validation_0-error:0.229535 validation_1-error:0.30597 [93] validation_0-error:0.229535 validation_1-error:0.30597 [94] validation_0-error:0.227929 validation_1-error:0.313433 [95] validation_0-error:0.226324 validation_1-error:0.313433 [96] validation_0-error:0.223114 validation_1-error:0.317164 [97] validation_0-error:0.223114 validation_1-error:0.317164 [98] validation_0-error:0.223114 validation_1-error:0.317164 [99] validation_0-error:0.223114 validation_1-error:0.317164 0.6977611940298507 LightGBM Python Wrapper 방식1234567891011121314151617181920212223242526272829303132333435import lightgbm as lgb from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_scoreimport seaborn as sns # tips 데이터셋 titanic = sns.load_dataset('titanic')X = titanic[['pclass', 'parch', 'fare']]y = titanic['survived']# 훈련데이터, 테스트 데이터 분리X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state=42)# XGBoost 코드와 유사하다. dtrain = lgb.Dataset(data = X_train, label = y_train)dtest = lgb.Dataset(data = X_test, label = y_test)params = {'max_depth':3, 'n_estimators':100, 'learning_rate': 0.1, 'objective':'binary', 'metric' : 'binary_error', 'num_boost_round' : 400, 'verbose' : 1} w_list = [dtrain, dtest]lgb_ml = lgb.train(params=params, train_set = dtrain,\\ early_stopping_rounds=100, valid_sets= w_list)pred_probs = lgb_ml.predict(X_test)y_pred=[1 if x &gt; 0.5 else 0 for x in pred_probs]# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred, y_test) /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias)) [1] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 Training until validation scores don't improve for 100 rounds. [2] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 [3] training's binary_error: 0.354735 valid_1's binary_error: 0.369403 [4] training's binary_error: 0.29695 valid_1's binary_error: 0.354478 [5] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [6] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [7] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [8] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [9] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [10] training's binary_error: 0.269663 valid_1's binary_error: 0.309701 [11] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [12] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [13] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [14] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [15] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [16] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [17] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [18] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [19] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [20] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [21] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [22] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [23] training's binary_error: 0.271268 valid_1's binary_error: 0.313433 [24] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [25] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [26] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [27] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [28] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [29] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [30] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [31] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [32] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [33] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [34] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [35] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [36] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [37] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [38] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [39] training's binary_error: 0.248796 valid_1's binary_error: 0.309701 [40] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [41] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [42] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [43] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [44] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [45] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [46] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [47] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [48] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [49] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [50] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [51] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [52] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [53] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [54] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [55] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [56] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [57] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [58] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [59] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [60] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [61] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [62] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [63] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [64] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [65] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [66] training's binary_error: 0.243981 valid_1's binary_error: 0.309701 [67] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [68] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [69] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [70] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [71] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [72] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [73] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [74] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [75] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [76] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [77] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [78] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [79] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [80] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [81] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [82] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [83] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [84] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [85] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [86] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [87] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [88] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [89] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [90] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [91] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [92] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [93] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [94] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [95] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [96] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [97] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [98] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [99] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [100] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [102] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [103] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [104] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [105] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [106] training's binary_error: 0.224719 valid_1's binary_error: 0.313433 [107] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [108] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [109] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [110] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [111] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [112] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [113] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [114] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [115] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [116] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [117] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [118] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [119] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [120] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [121] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [122] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [123] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [124] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [125] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [126] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [127] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [128] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [129] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [130] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [131] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [132] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [133] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [134] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [135] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [136] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [137] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [138] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [139] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [140] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [141] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [142] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [143] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [144] training's binary_error: 0.221509 valid_1's binary_error: 0.320896 [145] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [146] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [147] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [148] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [149] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [150] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [151] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [152] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [153] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [154] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [155] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [156] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [157] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [158] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [159] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [160] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [161] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [162] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [163] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [164] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [165] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [166] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [167] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [168] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [169] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [170] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [171] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [172] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [173] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [174] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [175] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [176] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [177] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [178] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [179] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [180] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [181] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [182] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [183] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [184] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [185] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [186] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [187] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [188] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [189] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [190] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [191] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [192] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [193] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [194] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [195] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [196] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [197] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [198] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [199] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [200] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [201] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 Early stopping, best iteration is: [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 0.6940298507462687 12345678910111213141516171819202122from lightgbm import LGBMClassifierfrom sklearn.metrics import accuracy_score# model w_list = [dtrain, dtest]model = LGBMClassifier(objective = 'binary', metric = 'binary_error', n_estimators=100, learning_rate=0.1, max_depth=3, num_boost_round = 400, random_state = 32)model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], verbose=1, early_stopping_rounds = 100)y_probas = model.predict_proba(X_test) y_pred=[1 if x &gt; 0.5 else 0 for x in y_probas[:, 1]] # 예측 라벨(0과 1로 예측)# 예측 라벨과 실제 라벨 사이의 정확도 측정accuracy_score(y_pred, y_test) [1] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 Training until validation scores don't improve for 100 rounds. [2] training's binary_error: 0.383628 valid_1's binary_error: 0.384328 [3] training's binary_error: 0.354735 valid_1's binary_error: 0.369403 [4] training's binary_error: 0.29695 valid_1's binary_error: 0.354478 [5] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [6] training's binary_error: 0.272873 valid_1's binary_error: 0.33209 [7] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [8] training's binary_error: 0.269663 valid_1's binary_error: 0.317164 [9] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [10] training's binary_error: 0.269663 valid_1's binary_error: 0.309701 [11] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [12] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [13] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [14] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [15] training's binary_error: 0.264848 valid_1's binary_error: 0.309701 [16] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [17] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [18] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [19] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [20] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [21] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [22] training's binary_error: 0.266453 valid_1's binary_error: 0.313433 [23] training's binary_error: 0.271268 valid_1's binary_error: 0.313433 [24] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [25] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [26] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [27] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [28] training's binary_error: 0.258427 valid_1's binary_error: 0.309701 [29] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [30] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [31] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [32] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [33] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [34] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [35] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [36] training's binary_error: 0.255217 valid_1's binary_error: 0.309701 [37] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [38] training's binary_error: 0.255217 valid_1's binary_error: 0.317164 [39] training's binary_error: 0.248796 valid_1's binary_error: 0.309701 [40] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [41] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [42] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [43] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [44] training's binary_error: 0.248796 valid_1's binary_error: 0.313433 [45] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [46] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [47] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [48] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [49] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [50] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [51] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [52] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [53] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [54] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [55] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [56] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [57] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [58] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [59] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [60] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [61] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [62] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [63] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [64] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [65] training's binary_error: 0.247191 valid_1's binary_error: 0.313433 [66] training's binary_error: 0.243981 valid_1's binary_error: 0.309701 [67] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [68] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [69] training's binary_error: 0.23435 valid_1's binary_error: 0.309701 [70] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [71] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [72] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [73] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [74] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [75] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [76] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [77] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [78] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [79] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [80] training's binary_error: 0.232745 valid_1's binary_error: 0.313433 [81] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [82] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [83] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [84] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [85] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [86] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [87] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [88] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [89] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [90] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [91] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [92] training's binary_error: 0.229535 valid_1's binary_error: 0.309701 [93] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [94] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [95] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [96] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [97] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [98] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [99] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [100] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [102] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 [103] training's binary_error: 0.227929 valid_1's binary_error: 0.309701 [104] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [105] training's binary_error: 0.221509 valid_1's binary_error: 0.317164 [106] training's binary_error: 0.224719 valid_1's binary_error: 0.313433 [107] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [108] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [109] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [110] training's binary_error: 0.224719 valid_1's binary_error: 0.317164 [111] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [112] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [113] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [114] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [115] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [116] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [117] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [118] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [119] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [120] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [121] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [122] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [123] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [124] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [125] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [126] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [127] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [128] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [129] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [130] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [131] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [132] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [133] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [134] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [135] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [136] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [137] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [138] training's binary_error: 0.219904 valid_1's binary_error: 0.309701 [139] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [140] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [141] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [142] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [143] training's binary_error: 0.223114 valid_1's binary_error: 0.309701 [144] training's binary_error: 0.221509 valid_1's binary_error: 0.320896 [145] training's binary_error: 0.223114 valid_1's binary_error: 0.313433 [146] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [147] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [148] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [149] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [150] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [151] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [152] training's binary_error: 0.221509 valid_1's binary_error: 0.313433 [153] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [154] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [155] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [156] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [157] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [158] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [159] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [160] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [161] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [162] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [163] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [164] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [165] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [166] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [167] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [168] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [169] training's binary_error: 0.219904 valid_1's binary_error: 0.324627 [170] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [171] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [172] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [173] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [174] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [175] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [176] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [177] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [178] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [179] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [180] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [181] training's binary_error: 0.221509 valid_1's binary_error: 0.328358 [182] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [183] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [184] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [185] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [186] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [187] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [188] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [189] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [190] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [191] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [192] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [193] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [194] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [195] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [196] training's binary_error: 0.216693 valid_1's binary_error: 0.320896 [197] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [198] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [199] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [200] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 [201] training's binary_error: 0.215088 valid_1's binary_error: 0.317164 Early stopping, best iteration is: [101] training's binary_error: 0.23114 valid_1's binary_error: 0.30597 /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias)) 0.6940298507462687","link":"/2022/07/06/day0706/"}],"tags":[],"categories":[]}